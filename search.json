[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An introduction to emuR",
    "section": "",
    "text": "Preface\nThis book is currently being built. It is an updated version of the materials for a course in Advanced Speech Technology taught in the MA Phonetics & Speech Processing at LMU Munich."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Initial setup",
    "section": "",
    "text": "Having trouble?\nIf you run into trouble with emuR, e.g. something doesn’t seem to be working like it’s supposed to and you’re fairly confident that the mistake isn’t yours, we strongly encourage you to alert the developers to this. You can do this via the issue tracker on GitHub. If you do so, please include as much information as possible, ideally providing a so-called reproducible example of code that produces the error."
  },
  {
    "objectID": "setup.html#required-set-up-for-the-course-on-emur",
    "href": "setup.html#required-set-up-for-the-course-on-emur",
    "title": "Initial setup",
    "section": "Required set up for the course on emuR",
    "text": "Required set up for the course on emuR\n\nIn order to use emuR, you must first download and install the program R. The latest version is currently 4.3.x (September 2023).\nAlso download and install the program RStudio, which is a so-called integrated development environment that makes working with R much much easier."
  },
  {
    "objectID": "setup.html#default-browser",
    "href": "setup.html#default-browser",
    "title": "Initial setup",
    "section": "Default browser",
    "text": "Default browser\nThe EMU-webApp for visualizing sound files and editing transcriptions should in theory work with most modern browsers, but you may run into unexpected problems if you use it with other browsers than Google Chrome. For this reason, we recommend that you change your standard browser to ensure that the EMU-webApp is opened in Chrome.\nIf you install Chrome, you can do this from the Settings tab as shown below:"
  },
  {
    "objectID": "setup.html#start-up-rstudio",
    "href": "setup.html#start-up-rstudio",
    "title": "Initial setup",
    "section": "Start up RStudio",
    "text": "Start up RStudio\nStart RStudio by clicking on the RStudio icon – it should look like this:"
  },
  {
    "objectID": "setup.html#create-a-directory-on-your-hard-drive",
    "href": "setup.html#create-a-directory-on-your-hard-drive",
    "title": "Initial setup",
    "section": "Create a directory on your hard drive",
    "text": "Create a directory on your hard drive\nCreate a directory on your hard drive to be used on this course. You can do this from R. What exactly to call the directory will depend on your computer – this will create a directory called ipsR on the desktop of my machine (which runs Windows):\n\ndir.create('C:/users/Rasmus/desktop/ipsR')\n\nIf I was a Mac user, I would have done it like this:\n\ndir.create('/Users/Rasmus/Desktop/ipsR')\n\nAlternatively you can just create a directory in the usual way – e.g. by right clicking anywhere on your desktop and selecting New directory or similar."
  },
  {
    "objectID": "setup.html#create-a-project",
    "href": "setup.html#create-a-project",
    "title": "Initial setup",
    "section": "Create a project",
    "text": "Create a project\nIn RStudio, go to File &gt; New Project.... Choose the directory you just created. See the figure below:\n\n\n\n\n\nYou only need to create the project once. When you start up RStudio the next time, the system should automatically open up inside this project that you created. If not, or if you closed the project, you can open it again:\n\n\n\n\n\nYou can get back to your default directory (if you were using R before then) any time by selecting Close project in the above menu."
  },
  {
    "objectID": "setup.html#install-packages",
    "href": "setup.html#install-packages",
    "title": "Initial setup",
    "section": "Install packages",
    "text": "Install packages\nIn the RStudio console window, run the following code to install packages needed for this course. It will take a few minutes. You only need to do this once.\n\ninstall.packages(c(\"Rcpp\", \"remotes\", \"knitr\", \n                   \"tidyverse\", \"magrittr\",\n                   \"rmarkdown\", \"emuR\", \"gridExtra\", \n                   \"broom\", \"pbkrtest\", \"wrassp\"))\n\n\n\n\n\n\n\nMore information: Installation of R packages\n\n\n\nShould the above result in the error message installation of package had non-zero exit status, then it means that installation has failed. For Windows, you might then additionally have to install Rtools. For iOS you might need to install/reset the XCode command-line tools. For this purpose, you can call the following code in the R console:\n\nsystem(\"xcode-select --install\")\n\nIf the installation of R packages still does not work, call the following code:\n\nsystem(\"xcode-select --reset\")\n\n\n\n\n\n\n\n\n\nThe emuhelpeR package\n\n\n\nThroughout the book there will be some tips on how to ease your work with emuR by using the functions of another package, emuhelpeR. This package is work-in-progress and the functions have not been tested thoroughly like those of emuR, and it’s not a requirement to use it. Because it’s work-in-progress, it is not (yet) available from the Comprehensive R Archive Network or CRAN, and can’t be installed with the install.packages() command. You can however install it from GitHub using the install_github() command from the devtools package.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"rpuggaardrode/emuhelpeR\")"
  },
  {
    "objectID": "setup.html#loading-libraries",
    "href": "setup.html#loading-libraries",
    "title": "Initial setup",
    "section": "Loading libraries",
    "text": "Loading libraries\nLoad in the emuR library like so:\n\nlibrary(emuR)\n\nVerify that everything works by entering the following commands to the console. The command create_emuRdemoData() downloads a demo database, and load_emuDB() loads in the database, in this case saving it as the object ae.\n\ncreate_emuRdemoData(dir = tempdir())\nae &lt;- load_emuDB(file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\"))\nserve(ae, useViewer=F)\n\nThe third command serve(ae, useViewer=F) should produce the following view in your browser:\n\nHave a look around and close the window to move on."
  },
  {
    "objectID": "setup.html#quitting-from-rstudio-and-r",
    "href": "setup.html#quitting-from-rstudio-and-r",
    "title": "Initial setup",
    "section": "Quitting from RStudio (and R)",
    "text": "Quitting from RStudio (and R)\nYou shut down RStudio by either clicking Session &gt; Quit Session in the main Rstudio toolbar or by running q() in the R console:\n\nq()\n\nYou will then be asked whether you want to save the workspace image. For working through the materials in this tutorial, you shouldn’t save the workspace image – i.e. (Don't save)."
  },
  {
    "objectID": "setup.html#learning-r",
    "href": "setup.html#learning-r",
    "title": "Initial setup",
    "section": "Learning R",
    "text": "Learning R\nA basic knowledge in R is a prerequisite for this tutorial. If you are unfamiliar with R then please start by working through these two sources:\n\nA short introduction to R by Raphael Winkelmann (in English)\nA more detailed introduction to R and the tidyverse package by Johanna Cronenberg in German or English\n\nThere is a very large and helpful R community online that will make learning R easier for you. Here are a few useful links and commands in case you get stuck:\n\nStack Overflow: A blog where you can often find an answer to your questions about R. The easiest way is to do an internet search of your question in English; a Stack Overflow member’s answer is usually included in the first search results.\nThe book R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023): Hadley Wickham is the chief programmer of the tidyverse. His books are very readable, well-structured, and entertaining.\nCheat sheets: These are PDFs that provide an overview of functions with explanations and examples in a very compact form. You will find some cheat sheets in the main RStudio toolbar via Help &gt; Cheat Sheets. The first three are especially useful.\nVignettes: For some packages there are so-called vignettes available. These are mostly HTMLs or PDFs that have been written by the package authors. You can search for vignettes using the following input to the console (this example finds a vignette for dplyr, one of the libraries from tidyverse):\n\n\nvignette(\"dplyr\")\n\n\nYou can get information about a function by entering its name in the RStudio Help toolbar. You’ll then get information about the function’s arguments and often some examples. You can also get help via the console by typing a question mark followed by the function name, as follows (e.g. here for getwd()):\n\n\n?getwd\nhelp(\"getwd\")\n\nR is first and foremost an environment developed for statistical computing. If you need more information about using statistics in R, the following are recommended:\n\nWinter (2019) is a recent book with excellent explanations to all major themes in inferential statistics.\nGries (2021) is a recent update of a classic which gives a methodical introduction to both R data structures and statistical models, starting with the very basics. It is useful for decision making about which model to use for which kind of question.\nSonderegger (2023) is a very recent book which introduces regression models and discusses in depth which problems you may run into when fitting them.\nAlthough the R code in Baayen (2008) is a bit out of date, the explanations and examples of statistics foundations are very helpful.\n\nBased on personal experience, we cannot recommend using large language models like ChatGPT to fix or explain R functions. Current versions of these models make many mistakes which can be difficult to solve without a solid understanding of R."
  },
  {
    "objectID": "intro.html#background",
    "href": "intro.html#background",
    "title": "1  A schematic overview of the Emu Speech Database Management System",
    "section": "1.1 Background",
    "text": "1.1 Background\nThe core idea behind Emu today is just the same as it was when it first launched in the mid-late 1980s at CSTR, Edinburgh University. At the time, it was known as APS which stood for Acoustic Phonetics in S.\nThe core idea behind Emu is:\n\nTo manage speech databases, which are collections of utterances consisting of signal (waveform, formant, f0 etc) and annotation (label) files.\nTo be able to query these databases: annotations can be extracted from the database. It should be straightforward to e.g. find all tokens of [i] in a database.\nThese queried lists of annotations can be further queried to get the corresponding signals (e.g. find the formants of the [i] tokens extracted at the previous step).\n\nThere have been some landmark changes to Emu since the APS–Edinburgh days of the 1980s.\n\nSince the 1990s, Emu’s query language has handled hierarchically structured annotations: see especially Cassidy and Harrington (2001). (The name Emu evolved out of Extended MUlti-dimensional, and of course because the developers were in Australia at the time).\n\n\n\n\n\n\nThe query language is still in use today and the only one of its kind in existence.\n\n\n\nThe author of the Emu query language is Steve Cassidy who can be seen here feeding an Emu ca 1996.\n\n\nThe point of hierarchical annotations is to be able to query annotations at one tier with respect to another. For example the previous query could be extended to:\n\nFind all [i] vowels in the first syllable of trisyllabic accented words, but only if they are preceded by a function word in any L% intonational phrase.\n\n\nSince the mid-00s, the Advanced Speech Signal Processor (ASSP) toolkit developed by Michel Scheffers of the IPdS, University of Kiel has been integrated into Emu (Bombien et al. 2006). ASSP since morphed into the R package wrassp ca. 2014.\nIn the past decade or so, the Emu engine has been completely overhauled by Raphael Winkelmann with many excellent new features (see also Winkelmann, Harrington, and Jänsch 2017), e.g.:\n\n\nEmu is launched and operates entirely within the R programming environment.\nAn interactive graphical user interface for analysing and visualising data: the Emu-webApp\nExtension of the query language to include regular expressions.\nFar more rapid access to extracting annotations and their signal files from the database."
  },
  {
    "objectID": "intro.html#the-emu-sdms",
    "href": "intro.html#the-emu-sdms",
    "title": "1  A schematic overview of the Emu Speech Database Management System",
    "section": "1.2 The Emu-SDMS",
    "text": "1.2 The Emu-SDMS\n\nThe following image provides a schematic overview of the Emu-SDMS.\n\nThe core components of the EMU-SDMS are:\n\nEmu databases. These are collections of sound files, annotation files, and signal files of a particular structure, with accompanying metadata.\nThe Emu WebApp. This is a web-based application allowing you to access their Emu databases: to view speech signals, and to annotate them in a fast, straightforward manner, among other features. You will be introduced to the Emu WebApp in Chapter 2.\nThe R library emuR, which allows you to manage the properties of a database, adding and removing annotation levels, adding and removing signals, querying the annotations and signals of a database, etc.\nThe R library wrassp, which is the signal processing engine of the system, including different pitch trackers, formant tracking, intensity tracking, and several other signal processing capabilities.\n\nMost of your work with the Emu-SDMS will be done in R – the WebApp is not controlled using R, but you will generally access it using R. emuR allows you to interface with other software though, for example to convert a collection of sound files and annotations made using Praat to an Emu database as we will see in Chapter 4, and to call the Munich Automatic Segmentation system as we will see in Chapter 5.\nThe R integration of Emu-SDMS has the major advantage of giving you immediate access to all the other tools that are available in R, such as the plotting capabilities of ggplot() and statistical modelling with e.g. lmer().\n\n\n\n\nBombien, Lasse, Steve Cassidy, Jonathan Harrington, Tina John, and Sallyanne Palethorpe. 2006. “Recent Developments in the Emu Speech Database System.” Proceedings of the Australian Speech Science and Technology Conference. https://www.phonetik.uni-muenchen.de/~jmh/papers/emusst06.pdf.\n\n\nCassidy, Steve, and Jonathan Harrington. 2001. “Multi-Level Annotation in the Emu Speech Database Management System.” Speech Communication 33 (1–2): 61–77. https://doi.org/10.1016/S0167-6393(00)00069-8.\n\n\nWinkelmann, Raphael, Jonathan Harrington, and Klaus Jänsch. 2017. “EMU-SDMS. Advanced Speech Database Management and Analysis in r.” Computer Speech & Language 45: 392–410. https://doi.org/10.1016/j.csl.2017.01.002."
  },
  {
    "objectID": "first_steps.html#preliminaries-and-loading-libraries",
    "href": "first_steps.html#preliminaries-and-loading-libraries",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.1 Preliminaries and loading libraries",
    "text": "2.1 Preliminaries and loading libraries\nFollow the setup instructions given in the setup chapter, i.e. download R and RStudio, create a directory on your computer where you will store files for this course, make a note of the directory path, create an R project that accesses this directory, and install all indicated packages. \nFor this and subsequent tutorials, access the tidyverse,magrittr, emuR, and wrassp libraries:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(emuR)\nlibrary(wrassp)"
  },
  {
    "objectID": "first_steps.html#accessing-an-existing-emu-speech-database",
    "href": "first_steps.html#accessing-an-existing-emu-speech-database",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.2 Accessing an existing Emu speech database",
    "text": "2.2 Accessing an existing Emu speech database\nFor a first look at the properties of Emu databases, we will use the function create_emuRdemodata() to download and access a “demo” database.\n\ncreate_emuRdemoData(dir = tempdir())\n\nThe option dir = tempdir() tells R to store the demo database in a temporary directory, the path of which is determined automatically by the tempdir() function.\nWe can now use the command list.dirs() to have a look inside the temporary directory and see which directories are in there. (recursive=FALSE tells R not to return the names of all subdirectories, and full.names=FALSE tells R not to return the full paths to directories).\n\nlist.dirs(tempdir(), recursive=FALSE, full.names=FALSE)\n\n[1] \"emuR_demoData\"\n\n\nOur demo data is stored in the first subdirectory, emuR_demoData. Let’s have a look at what is stored in there.\n\n#store path name as an object\npath.emuDB &lt;- file.path(tempdir(), \"emuR_demoData\")\nlist.dirs(path.emuDB, recursive=FALSE, full.names=FALSE)\n\n[1] \"ae_emuDB\"            \"BPF_collection\"      \"legacy_ae\"          \n[4] \"TextGrid_collection\" \"txt_collection\"     \n\n\nThere are some folders containing collections of sound files and annotation files. (In Chapter 4 we will see how to convert an existing collection of sound files and TextGrids to an Emu database). There is also a folder called ae_emuDB, which contains our actual demo database. The database is called ae, and the string _emuDB is always appended to Emu databases. The full path to this Emu database can be accessed with the file.path() function, like so:\n\nfile.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\n\n[1] \"C:\\\\Users\\\\rasmu\\\\AppData\\\\Local\\\\Temp\\\\RtmpKESI5u/emuR_demoData/ae_emuDB\"\n\n\nIn order to see the files that are physically stored in the ae_emuDB, first save the path name and then use the function list.files():\n\npath.ae &lt;- file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\nlist.files(path.ae)\n\n[1] \"0000_ses\"         \"ae_DBconfig.json\"\n\n\nThe file ae_DBconfig.json is a template that stores information about the defining properties of the ae database; see Section 2.3. The utterances are in this case all stored in the directory 0000_ses. To look inside this directory:\n\nlist.files(file.path(path.ae, \"0000_ses\"))\n\n[1] \"msajc003_bndl\" \"msajc010_bndl\" \"msajc012_bndl\" \"msajc015_bndl\"\n[5] \"msajc022_bndl\" \"msajc023_bndl\" \"msajc057_bndl\"\n\n\nThis shows that the 0000_ses contains 7 directories of so-called bundles with the _bndl suffix. Emu organises things such that all the files that belong to the same utterance are always in the same bundle. The utterance identifier precedes _. Thus in this case, it is clear from the output above that there are 7 utterances whose identifiers are msajc003, msajc010, msajc012, msajc015, msajc023, msajc057. We can again use list.files() in combination with file.path() to see what files there are for any bundle, in this case for msajc003:\n\nlist.files(file.path(path.ae, \"0000_ses\", \"msajc003_bndl\"))\n\n[1] \"msajc003.dft\"        \"msajc003.fms\"        \"msajc003.wav\"       \n[4] \"msajc003_annot.json\"\n\n\nThis shows that msajc003 has the following files:\n\n_annot.json: this stores information about the annotations.\n.dft: this contains a derived signal file of DFT (spectral) data obtained from the speech waveform.\n.fms: this contains formant data also obtained from the speech waveform.\n.wav: this contains the sound itself.\n\n.dft and .fms are just examples of the sorts of signal files that can be stored in an Emu database – this could also have been signals like pitch tracks or intensity, but for the advanced user, essentially any signal can be stored in an Emu database.\n\nIn order to access the ae emuDB in R, the above path needs to be stored and passed to the function load_emuDB() that reads the database into R. The output of load_emuDB() should always be assigned to an object in R, or you will not be able to access it later. Here we store it as ae.\n\n# store the above path name\npath2ae &lt;- file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\nae &lt;- load_emuDB(path2ae, verbose=FALSE)\n\n\n\n\n\n\n\nWhat does verbose=FALSE mean?\n\n\n\nThroughout this book you will often see us use the option verbose=FALSE when calling functions from emuR. By default emuR is pretty talkative (or verbose), meaning it will print a lot of information in the R console about the status of whatever task it is performing. This is usually quite nice, and you may want to avoid the verbose=FALSE option if you are coding along in your own R session – we mostly keep it quiet here because those messages would otherwise clutter up the book."
  },
  {
    "objectID": "first_steps.html#sec-defprops",
    "href": "first_steps.html#sec-defprops",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.3 Some defining properties of an Emu database",
    "text": "2.3 Some defining properties of an Emu database\nThe well-known function summary() can also be used with an Emu database to summarise its salient attributes:\n\nsummary(ae)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ae \nUUID:    0fc618dc-8980-414d-8c7a-144a649ce199 \nDirectory:   C:\\Users\\rasmu\\AppData\\Local\\Temp\\RtmpKESI5u\\emuR_demoData\\ae_emuDB \nSession count: 1 \nBundle count: 7 \nAnnotation item count:  736 \nLabel count:  844 \nLink count:  785 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\n name columnName fileExtension fileFormat\n dft  dft        dft           ssff      \n fm   fm         fms           ssff      \n\n\n── Level definitions ──\n\n\n name         type    nrOfAttrDefs attrDefNames       \n Utterance    ITEM    1            Utterance;         \n Intonational ITEM    1            Intonational;      \n Intermediate ITEM    1            Intermediate;      \n Word         ITEM    3            Word; Accent; Text;\n Syllable     ITEM    1            Syllable;          \n Phoneme      ITEM    1            Phoneme;           \n Phonetic     SEGMENT 1            Phonetic;          \n Tone         EVENT   1            Tone;              \n Foot         ITEM    1            Foot;              \n\n\n── Link definitions ──\n\n\n type         superlevelName sublevelName\n ONE_TO_MANY  Utterance      Intonational\n ONE_TO_MANY  Intonational   Intermediate\n ONE_TO_MANY  Intermediate   Word        \n ONE_TO_MANY  Word           Syllable    \n ONE_TO_MANY  Syllable       Phoneme     \n MANY_TO_MANY Phoneme        Phonetic    \n ONE_TO_MANY  Syllable       Tone        \n ONE_TO_MANY  Intonational   Foot        \n ONE_TO_MANY  Foot           Syllable    \n\n\nThe directory shows the path where the directory is located.\nBundle count shows how many utterances there are in the database. We can query their names with the function list_bundles.\n\nlist_bundles(ae)\n\n\n\n  \n\n\n\nThe SSFF track definitions show which signals are currently available in the database apart from the waveforms. For this database there are signals of type dft (discrete Fourier transform, i.e. spectral data) and of type fm (formant). They have extensions .dft and .fms. This information is also given by:\n\nlist_ssffTrackDefinitions(ae)\n\n\n\n  \n\n\n\nThe Level definitions show the available annotation levels of the database (equivalent to tiers in Praat). This information is also given by:\n\nlist_levelDefinitions(ae)\n\n\n\n  \n\n\n\nThis requires some unpacking!"
  },
  {
    "objectID": "first_steps.html#types-of-annotation-levels-and-links",
    "href": "first_steps.html#types-of-annotation-levels-and-links",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.4 Types of annotation levels and links",
    "text": "2.4 Types of annotation levels and links\nAs the above shows, annotation levels can be of three types: ITEM, SEGMENT, and EVENT. The annotations of ITEM levels inherit their time information from the (typically) SEGMENT levels that they dominate. In SEGMENT levels, each annotation has a start and end time, equivalent to segment tiers in Praat. In an EVENT level, each annotation is defined by a single point in time, equivalent to point tiers in Praat. A level can also be associated with one or more ATTRIBUTE levels. This information is also given by the function list_attributeDefinitions() with the database name as the first argument, and the level to be queried for attributes as the second. For example:\n\nlist_attributeDefinitions(ae, \"Word\")\n\n\n\n  \n\n\n\nThis shows that the levels Accent and Text are attributes of Word. The annotations of an ATTRIBUTE level always have identical times to those of the main level with which they are associated (thus the annotations of the Accent level have identical start and end times to those of the Word level). An ATTRIBUTE level is often used to provide additional information about annotations. In the ae database, the annotations in the Word level consist entirely of either C (content word) or F (function word). The annotations at the Text level are used to provide the orthography for each content or function word annotation; and the annotations of the Accent level are used to mark whether or not a word is prosodically accented.\nThe information in Link definitions of summary(ae) shows how the levels are associated with each other. This information is also provided by the function list_linkDefinitions():\n\nlist_linkDefinitions(ae)\n\n\n\n  \n\n\n\nIn the Emu system, annotation levels can be (but need not be) hierarchically organised with respect to each other. One reason for the hierarchical organisation of levels is to allow annotations to inherit times if these are predictable.\nIn the left part of Figure 2.1 for example, the start and end times of the annotation seen in the Text level are completely predictable from the annotations at the Phonetic level that is dominated by Text. Text is an item level, Phonetic is a segment level as indicated by (S). Text dominates Phonetic as shown by the vertical downward arrow. Compatibly, the Emu system provides a way for the start and end times of seen to be inherited from the annotations at the hierarchically lower SEGMENT level Phonetic. Because Text is an ITEM level that dominates Phonetic which is a SEGMENT level, annotations at the Text level inherit their times from Phonetic. Consequently, the duration of seen is t4 – t1. Text and Phonetic stand in a ONE_TO_MANY association (as signified by the downward arrow) because an annotation at the Text level can be associated with one or more annotations at the Phonetic level, but not vice versa.\n\n\n\nFigure 2.1: Example relations between annotation levels.\n\n\nOn the right, Phoneme and Phonetic stand in a MANY_TO_MANY relationship (as indicated by the double arrow) because an annotation at the Phoneme tier can map to more than one annotation at the Phonetic level and vice versa. In this hypothetical example of an annotation of the second syllable of a word like region, the single affricate annotation /dZ/ at the Phoneme level maps to a sequence of [d] and [Z] annotations at the Phonetic level, while the single annotation of the syllabic [n] at the Phonetic level maps to a sequence of annotations /@n/ at the Phoneme tier. Note that /@/ and /n/ inherit the same start and end times and therefore have the same duration of t4 – t3, i.e. they overlap with each other in time completely.\nAnother reason for the hierarchical organisation of annotation levels is that it allows users to query the database in order to obtain annotations at one tier with respect to another (e.g., all orthographic annotations of the vowels in the database; all H* pitch accents in an intermediate phrase, etc.). Without this linkage, these types of queries would not be possible.\nEmu allows quite a flexible configuration of annotation levels. The type of configuration can be defined by the user and will depend on the types of information that the user wants to be able to extract from the database. The configuration of annotation levels for the currently loaded ae database is shown in Figure 2.2.\n\n\n\nFigure 2.2: The links between the annotation levels of the ae database. ITEM levels are unmarked, SEGMENT levels are marked with (S) and EVENT levels with (E). ATTRIBUTE levels have no arrow between them (thus Text and Accent are attribute levels of Word). A downward arrow signifies domination in a ONE_TO_MANY relationship; a double arrow signifies domination in a MANY_TO_MANY relationship.\n\n\nInherited times percolate up through the tree from levels with associated time information, i.e. from SEGMENT and EVENT levels upwards through ITEM levels. Thus, Phoneme is an item level which inherits its times from the SEGMENT level Phonetic. Word inherits its times from Syllable which inherits its times from Phoneme (and therefore from Phonetic) and so on all the way up to the top level Utterance. Sometimes, levels can inherit more than one set of times. In Figure 2.2, Syllable inherits times both from Phonetic (S) and from Tone (E). For the same reason, all the levels that dominate Syllable (including Foot) inherit these two sets of times.\nAny two annotation levels on the same path can be queried with respect to each other, where a path is defined as levels connected by arrows. There are in fact four paths in the configuration:\n\nUtterance → Intonational → Intermediate → Word → Syllable → Phoneme ↔︎ Phonetic (S)\nUtterance → Intonational → Root → Syllable → Phoneme ↔︎ Phonetic (S)\nUtterance → Intonational → Syllable → Tone (E)\nUtterance → Intonational → Intermediate → Word → Syllable → Tone (E)\n\nFrom (1–4), it becomes clear that e.g. annotations of the Syllable tier can be queried with respect to Tone (e.g. which syllables contain an H* tone?) and vice versa (e.g. are there any H* tones in weak syllables?); or annotations at the Intermediate tier can be queried with respect to Word (how many words are there in an L- intermediate phrase?) and vice versa (which words are in an L- intermediate phrase?). But e.g. Phoneme and Tone can’t be queried with respect to each other, and nor can Word and Foot, because they aren’t on the same path."
  },
  {
    "objectID": "first_steps.html#viewing-and-annotating-an-emu-database",
    "href": "first_steps.html#viewing-and-annotating-an-emu-database",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.5 Viewing and annotating an Emu database",
    "text": "2.5 Viewing and annotating an Emu database\nAn Emu database can be viewed and annotated in at least two ways using the serve() function. Simply passing the database object to serve() will open the Emu database within the RStudio graphics window:\n\nserve(ae)\n\nYou’ll probably usually prefer to watch the database in a larger window; if you include the argument useViewer = FALSE, it’ll open in your default browser instead (which should preferably be Chrome). Alternatively you can click the Show in new window bottom in the RStudio viewer.\n\nserve(ae, useViewer = FALSE)\n\n\n\n\nFigure 2.3: The ae database\n\n\n\n\nThe properties of the Emu WebApp are covered in a fair amount of detail in Chapter 3. For now, we will just mention a few basic properties of what we see when we view the ae database:\n\nThere are, as mentioned before, 7 utterances.\nThe database displays two types of signals: the waveform, and the spectrogram below it.\n\nThis information about which signals are being displayed is also given by the function get_signalCanvasesOrder(). This function takes the obligatory argument perspectiveName, which should be \"default\" unless you have created custom signal canvases (discussed further below).\n\nget_signalCanvasesOrder(ae, perspectiveName = \"default\")\n\n[1] \"OSCI\" \"SPEC\"\n\n\nOSCI is the waveform and SPEC the spectrogram. These can be changed with the order argument in the function set_signalCanvasesOrder(). For example, if we want to display only the spectrogram, we can do it like so:\n\nset_signalCanvasesOrder(ae, perspectiveName = \"default\", order = \"SPEC\")\n\nIf you serve() the database again, you should see only the spectrogram:\n\nserve(ae, useViewer = F)\n\nTo restore the original order, call set_signalCanvasesOrder() again:\n\nset_signalCanvasesOrder(ae, perspectiveName = \"default\", \n                        order = c(\"OSCI\", \"SPEC\"))\n\n\n\n\n\n\n\nAdding signal canvases with emuhelpeR\n\n\n\nOne of the most common use cases of set_signalCanvasesOrder() is to add a signal (such as the formant track fm) to the default perspective for a quick look at it. This is a little cumbersome with set_signalCanvasesOrder(), but a helper function add_signal_canvas() can be found in the emuhelpeR package. This function takes just two arguments: the database object, and the name(s) of signal tracks to add to the default perspective.\n\nlibrary(emuhelpeR)\nadd_signal_canvas(ae, add=\"fm\")\n\nTo reset the default perspective so that it shows just a waveform and spectrogram, emuhelpeR has the helper function reset_signal_canvas:\n\nreset_signal_canvas(ae)\n\n\n\nEmu will only ever display time levels with signals (in this case there are two: Phonetic and Tone that are SEGMENT and EVENT tiers respectively). The time levels to be displayed and their order can be shown and changed with the functions get_levelCanvasesOrder() and set_levelCanvasesOrder().\n\nget_levelCanvasesOrder(ae, perspectiveName = \"default\")\n\n[1] \"Phonetic\" \"Tone\"    \n\n\nWe can change the order of the Phonetic and Tone levels like so:\n\nset_levelCanvasesOrder(ae, perspectiveName = \"default\", \n                       order = c(\"Tone\", \"Phonetic\"))\n\nOr display only the tone level, like so:\n\nset_levelCanvasesOrder(ae, perspectiveName = \"default\", order = \"Tone\")\n\nThe ITEM annotation levels can all be seen in the hierarchy view. The four paths identified earlier are visible when clicking on the triangle on the far right (Figure 2.3). Clicking the triangle can also be used to change to another path. The attribute levels can be seen by clicking on one of the level names displayed at the top – e.g. click on Word to show the attribute levels Text and Accent with which the Word level is associated.\n\n\n\n\n\n\nemuR and pipes\n\n\n\nIf you are a regular user of the tidyverse libraries, you are probably familiar with the pipe operator %&gt;%. You may have noticed that the first argument of emuR functions is almost always the emuDBhandle. This means that you can pipe the database object into these functions, for example\n\nae %&gt;% list_ssffTrackDefinitions\n\n\n\n  \n\n\nae %&gt;% list_levelDefinitions\n\n\n\n  \n\n\nae %&gt;% get_levelCanvasesOrder(\"default\")\n\n[1] \"Tone\"\n\n\nSimilarly, you can view a database by calling ae %&gt;% serve."
  },
  {
    "objectID": "first_steps.html#functions-introduced-in-this-chapter",
    "href": "first_steps.html#functions-introduced-in-this-chapter",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.6 Functions introduced in this chapter",
    "text": "2.6 Functions introduced in this chapter\n\ncreate_emuRdemoData(): downloads an online demo database\nlist.dirs() and list.files(): lists the directories or files within a directory on your computer\nfile.path(): returns the location of a directory on your computer\nload_emuDB(): loads an emuDB into the R environment\nsummary() lists the salient contents of an emuDB\nlist_bundles(): lists the so-called bundles of an emuDB\nlist_levelDefinitions(): lists the annotation levels of tiers of an emuDB\nlist_attributeDefinitions(): lists the attribute tiers of an annotation tier\nlist_linkDefinitions(): lists the links between the tiers of an emuDB\nlist_ssffTrackDefinitions(): lists the available signal files of an emuDB\nserve(): view and annotate an emuDB\nget_signalCanvasesOrder(): shows what signals are being displayed when serve() is launched.\nset_signalCanvasesOrder(): changes the signals to be displayed.\nadd_signal_canvas(): adds one or more signals to the default display perspective (requires the emuhelpeR package)\nreset_signal_canvas(): resets the default display perspective so it shows the waveform and spectrogram (requires the emuhelpeR package)\nget_levelCanvasesOrder(): shows what annotation levels are being displayed when serve() is launched.\nset_levelCanvasesOrder(): changes the annotation levels to be displayed."
  },
  {
    "objectID": "wav2emu.html#preliminaries",
    "href": "wav2emu.html#preliminaries",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.1 Preliminaries",
    "text": "3.1 Preliminaries\nIf you want to code along with this chapter, you should download and unzip the file testsample (accessible here). Store the testsample folder in the ipsR directory you made when you did the initial setup for the tutorial. Also create a directory called emu_databases on your machine in the ipsR directory. The directory should now look like this:\n\n\n\n\n\nNow start up R and load the following libraries:\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\n\nIn R, store the path to the directory testsample under the name sourceDir in the following way:\n\nsourceDir &lt;- \"./testsample\"\n\nAnd also store in R the path to emu_databases as targetDir:\n\ntargetDir &lt;- \"./emu_databases\"\n\nNote that if you source this tutorial directly from GitHub, these documents and file structures are already available in the tutorial R project."
  },
  {
    "objectID": "wav2emu.html#creating-an-emu-database-from-scratch",
    "href": "wav2emu.html#creating-an-emu-database-from-scratch",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.2 Creating an Emu database from scratch",
    "text": "3.2 Creating an Emu database from scratch\nIn this chapter, we will see how to build an Emu database from scratch.\nOne of the test samples you downloaded earlier is called german. Store the path to this directory as below:\n\npath.german &lt;- file.path(sourceDir, \"german\")\n\nThis database contains the following sound files (we use the pattern = \"*wav\" argument to get only files where the name ends in wav).\n\nlist.files(path.german, pattern = \"*wav\")\n\n[1] \"K01BE001.wav\" \"K01BE002.wav\"\n\n\nWe create an empty database using the create_emuDB() command:\n\ncreate_emuDB(name = \"german\", \n             targetDir = targetDir)\n\nThis can now be loaded in using the load_emuDB() command we saw in the previous chapter.\n\ngerman_DB &lt;- load_emuDB(file.path(targetDir, \"german_emuDB\"))\n\nWhat’s in the database? Nothing!\n\nsummary(german_DB)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    german \nUUID:    7ae2a927-684f-4b94-9409-ccd3ab5a06f2 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\german_emuDB \nSession count: 0 \nBundle count: 0 \nAnnotation item count:  0 \nLabel count:  0 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nWe can import the WAV files in our German test sample into the database using the import_mediaFiles() command:\n\nimport_mediaFiles(german_DB, dir = path.german, verbose = FALSE)\n\nWhat’s in the database now? Not nothing!\n\nsummary(german_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    german \nUUID:    7ae2a927-684f-4b94-9409-ccd3ab5a06f2 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\german_emuDB \nSession count: 1 \nBundle count: 2 \nAnnotation item count:  0 \nLabel count:  0 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nThe following bundles are available:\n\nlist_bundles(german_DB)\n\n\n\n  \n\n\n\nNote that all bundles are associated with a session. If the sound files all come from a single directory, they are assigned to a session called 0000. If our testsample/german directory had had multiple subdirectories, these would be assigned to different sessions with the same names as the subdirectories.\nYou can now serve() the database and have a look at the data.\n\nserve(german_DB, useViewer = FALSE)\n\nWe’re almost always interested in adding annotations to our databases. You can add an annotation level using the add_levelDefinition() function. Annotation levels need a name (we call this one Phon) and a type. The type is SEGMENT because we want to add annotation units with start and end times; the different types of annotation levels (SEGMENT, ITEM, and EVENT) were presented in the previous chapter.\n\nadd_levelDefinition(german_DB, \n                    name = \"Phon\", \n                    type = \"SEGMENT\",\n                    verbose = FALSE)\n\nWill this new annotation level be shown when we serve() the database again? We can check this with get_levelCanvasesOrder(), as seen in the previous chapter\n\nget_levelCanvasesOrder(german_DB, \n                       perspectiveName = \"default\")\n\nNULL\n\n\nNope! New annotation levels are not shown by default, but need to be explicitly added to a perspective with the function set_levelCanvasesOrder().\n\nset_levelCanvasesOrder(german_DB, \n                       perspectiveName = \"default\", \n                       order = \"Phon\")\n\nIf we now serve() the database, the Phon level should be visible.\n\nserve(german_DB, useViewer = FALSE)\n\nIt’s still empty though. Try to annotate some segments as in Figure 3.1, e.g. /O, a/ for the vowels of Sonne and lacht in K01BE002 and save the annotations.\n\n\n\nFigure 3.1: An utterance with two annotations from german_emuDB.\n\n\n\n\n\n\n\n\nSome information about annotating in Emu\n\n\n\nThe procedure for entering annotations into Emu may not be immediately intuitive especially if you are used to doing this in Praat. Once you get the hang of it, though, it is very fast!\nCreating an interval can be done as in Praat. Place your cursor at the desired start point of an interval, then left click your mouse and hold it while dragging your cursor to the desired end point of an interval, and then let go of the clicker. (If this sounds complicated – it isn’t! Don’t overthink it, just think of it as the usual way to mark some interval with your mouse.) Now hit the enter key on your keyboard. This should draw two vertical lines signaling the boundaries of your segment.\nIf you don’t want to create an interval but simply one boundary, simply place your cursor somewhere in the spectrogram or waveform, left click your mouse, and hit the enter key. This should draw a single vertical line.\nDeleting a boundary can be done by simply placing your cursor near the boundary you want to delete. You should see it turn from right to blue – this means that it is selected, you don’t need to click it. Now hit the backspace key on your keyboard. This should delete the boundary, and if relevant, it will combine the text in two adjacent intervals. If you want to delete an interval including the annotations in it, left click somewhere within the interval and press Shift + Backspace. This will expand the surrounding intervals.\nTo enter a label, left click inside an interval, and hit the enter key. Alternatively you can use your arrow keys to move left and right between intervals, and hit the enter keys when you have found the one you want to edit. If you have just created a boundary, it is automatically selected, and you can hit enter right away to start entering a label. The interval will now turn bright yellow, and you can type in text. When you are done typing, hit the enter key again.\nTo move a boundary, simply place your cursor near a boundary as explained above, hold down the Shift key, and move your cursor left to right. Let go of the Shift key when the boundary is in the right position.\nTo save an annotation, either hit Shift + S, or left click on the Save icon next to the bundle name in the left side your Emu window; it will have turned red.\n\n\nAssuming you have managed to annotate the segments as in Figure 3.1 above, the segments should now be accessible in R with the function query(). (As mentioned in Chapter 1, the Emu query language is one-of-a-kind and very flexible! It will be discussed in much more detail in Chapter 7. Here we query the regular expression .* in the Phon level, which will return all intervals in that annotation level. Note that the data frame returned by query() contains a column db_uuid which we’ll remove for now, simply to make the output more readable.)\n\nquery(german_DB, \"Phon =~ .*\") %&gt;% select(-db_uuid)"
  },
  {
    "objectID": "wav2emu.html#adding-word-annotations",
    "href": "wav2emu.html#adding-word-annotations",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.3 Adding word annotations",
    "text": "3.3 Adding word annotations\nThe next task is to add orthographic labels as ITEM annotations. This should be done if either (a) the start and end times are of no concern and/or (b) a word’s start and end time are inherited from segments. There are three steps:\nAdd a new level to the database using the function add_levelDefinition(). Here we add one with the name ORT:\n\nadd_levelDefinition(german_DB, \n                    name = \"ORT\", \n                    type = \"ITEM\",\n                    verbose = FALSE)\n\nDefine how it is linked with a time-based level, in this case with Phon. We do this with the function add_linkDefinition(). The different types of link were discussed in Chapter 2.\n\nadd_linkDefinition(german_DB, \n                   type = \"ONE_TO_MANY\", \n                   superlevelName = \"ORT\", \n                   sublevelName = \"Phon\")\n\nFinally, we can check that the hierarchical links are as expected using the function list_linkDefinitions().\n\nlist_linkDefinitions(german_DB)\n\n\n\n  \n\n\n\n\n3.3.1 Adding ITEM annotations via the EMU-webApp\nIf you serve() the database, you can add ITEM annotations interactively. Open the hierarchy window for utterance K01BE001, and click on the blue and white + sign next to ORT. Each time you do so, a node appears. You can enter annotation text for any node by positioning the mouse over it and then left click to bring up a green rectangle as in the figure below. You can type the text corresponding to your ITEM annotation into this rectangle, and then hit enter. To delete a node, move the mouse over it and hit the y key. Further details can be seen in the figure below.\n\nIf you have done something as in the above figure, you should be able to access the annotations in R using query() as we saw above. Notice the NA under start and end times. This is because they are timeless, i.e. unlinked to any annotations of a time (SEGMENT or EVENT) level\n\nquery(german_DB, \"ORT =~ .*\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\n\n\n3.3.2 Adding ITEM annotations via the emuR\nYou can also add timeless annotations (in this case to the ORT level) with the function create_itemsInLevel(). This requires passing a data frame with certain specific requirements to the itemsToCreate argument. This data frame should have the columns labels, session, bundle, level, start_item_seq_idx, and attribute, with one row for each annotation to be added. In this example, the words die Sonne lacht will be added to the second utterance.\nLet’s make a data frame with this information step-by-step. First, we store the sentence as a vector w with a string corresponding to each word:\n\nw &lt;- c(\"die\", \"Sonne\", \"lacht\")\n\nWhat session do the labels belong to? In this case there is only one session, 0000. Make a vector with the same length as w which repeats the session name.\n\nsess &lt;- rep(\"0000\", length(w))\n\nWhat bundle do the labels belong to?\n\nbundle &lt;- rep(\"K01BE002\", length(w))\n\nWhat’s the name of the annotation level? This will be ORT as created above, and in this case the attribute name is identical to the level name (see Chapter 1 for more details).\n\nlev &lt;- rep(\"ORT\", length(w))\n\nWhat order do the annotations occur in? This is 1, 2, 3 for die Sonne lacht.\n\ninds &lt;- 1:length(w)\n\nPut all the above information into a data frame as follows:\n\nnewItems_ORT &lt;- data.frame(session = sess, \n                          bundle = bundle, \n                          level = lev, \n                          start_item_seq_idx = inds,\n                          attribute = lev, \n                          labels = w,\n                          stringsAsFactors = FALSE)\nnewItems_ORT\n\n\n\n  \n\n\n\nAdd these word annotations to the database:\n\ncreate_itemsInLevel(german_DB, newItems_ORT, \n                    verbose = FALSE)\n\n\n\n\n\n\n\nUsing create_itemsInLevel() with SEGMENT or EVENT levels\n\n\n\nYou can also add annotation items for annotation levels with time information, i.e. SEGMENT or EVENT levels. This requires a data frame with the same columns as newItems_ORT which we created above, but instead of having numeric indexes in the column start_item_seq_idx, this column should contain the start time in ms of the selected boundary. (In the case of an EVENT level, this should be called start instead of start_item_seq_idx).\nIf you’re creating annotation items programmatically in a SEGMENT tier in this way, you’ll likely also want to add an end time. Somewhat counterintuitively, you do this by adding another row to the data frame with an empty label. If we wanted to use create_itemsInLevel() to create the vowel labels in K01BE002 that we just made manually, we would do it like this:\n\nw &lt;- c(\"O\", \"\", \"a\", \"\")\nsess &lt;- rep(\"0000\", length(w))\nbundle &lt;- rep(\"K01BE002\", length(w))\nlev &lt;- rep(\"Phon\", length(w))\ntimes &lt;- c(1048.5, 1111.7, 1275, 1391.1)\n\nnewItems_Phon = data.frame(session = sess, \n                          bundle = bundle, \n                          level = lev, \n                          start_item_seq_idx = times,\n                          attribute = lev, \n                          labels = w,\n                          stringsAsFactors = FALSE)\nnewItems_Phon\n\n\n\n  \n\n\n\nThe newItems_Phon can now be passed onto the create_itemsInLevel() function as we saw above. This of course requires us to know our boundary times already, but one can easily imagine a situation where signal processing tools are used to find landmarks in sound files, and these landmarks are then used to create boundaries.\n\n\n\nIf we use query() as above, we should now see words in both bundles:\n\nquery(german_DB, \"ORT =~ .*\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nHave a look at the database again:\n\nserve(german_DB, useViewer = FALSE)\n\nLook at the hierarchy for the second utterance. The word annotations in the ORT level should now be visible. These can be linked manually in the Emu-WebApp so that the word labels are accessible in emuR. Please see section 9.2.2 of the Emu SDMS manual for details on how to annotate hierarchically.\n\n\n\n\n\n\nSome information about annotating hierarchically\n\n\n\nAdding hierarchical links and annotations is not difficult. The present task is to add links from O and from a at the Phon level to Sonne and to lacht respectively at the ORT level To do this for the first of these, hover the mouse over Sonne (the node will turn blue), hold down the shift key, and sweep the mouse to O (whose node will also turn blue), release the shift key, and the link is made. If you want to delete the link, hover the mouse over it (the link will then turn bright yellow) and hit backspace.\nYou can add new nodes at the ORT level as follows: Move the mouse over lacht, type n and hit enter. This will create a new node before lacht (between Sonne and lacht). If you enter m instead of n in the above operation, a new node will be created after lacht. To edit or play a node at the ORT tier, left click on the node. You can enter or modify text in the green panel. To denote a node at the ORT level, hover over it and enter y. This is shown in the figure below.\n\n\n\nIf you have annotated as in the above figure and saved it, the words and their times will be accessible (note that the times at the ORT level are the same as the times at the Phon level, because each word only dominates one segment (and inherits its times from those). Note also that die has no times, because it hasn’t been linked to any annotations at the Phon level. We can check this with query():\n\nquery(german_DB, \"ORT =~ .*\", bundlePattern = \"K01BE002\") %&gt;% select(-db_uuid)"
  },
  {
    "objectID": "wav2emu.html#functions-introduced-in-this-chapter",
    "href": "wav2emu.html#functions-introduced-in-this-chapter",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.4 Functions introduced in this chapter",
    "text": "3.4 Functions introduced in this chapter\n\ncreate_emuDB(): makes an emuDB from scratch\nimport_mediaFiles(): adds sound files to an emuDB\ncreate_itemsInLevel(): allows for annotating an emuDB programmatically from R.\nquery(): queries the annotation levels in an emuDB. This will be discussed in much more detail in Chapter 7."
  },
  {
    "objectID": "praat2emu.html#objective-and-preliminaries",
    "href": "praat2emu.html#objective-and-preliminaries",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.1 Objective and preliminaries",
    "text": "4.1 Objective and preliminaries\nThe aim of this chapter is to show how to get from a Praat .TextGrid format to an Emu database format. We will show how to convert pairs of .wav and .TextGrid files to an Emu database, how to generate pitch tracks over the sound files, how to add annotation levels in the style of Praat’s point tiers, and finally how to convert the flat annotation structure of Praat TextGrids into the layered annotation structure of an Emu database (see Chapter 1).\nFigure 4.1 exemplifies an annotated sound file in Praat with pitch overlaid on the spectrogram. Below this, we see the Emu-webApp we want to achieve, with a visible pitch track, annotations, and a layered annotation structure.\n\n\n\nFigure 4.1: An utterance fragment in Praat and in Emu.\n\n\nThe assumption is that you already have an R project called ipsR and that it contains the directories emu_databases and testsample. If this is not the case, please go back and follow the preliminaries chapter.\n\nStart up R in the project you are using for this course and load the usual libraries.\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\n\nIn R, store the path to the directory testsample as sourceDir in the following way:\n\nsourceDir &lt;- \"./testsample\"\n\nAnd also store in R the path to emu_databases as targetDir:\n\ntargetDir &lt;- \"./emu_databases\""
  },
  {
    "objectID": "praat2emu.html#converting-praat-textgrids",
    "href": "praat2emu.html#converting-praat-textgrids",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.2 Converting Praat TextGrids",
    "text": "4.2 Converting Praat TextGrids\nThe directory testsample/praat on your computer contains a Praat-style database with .wav files and .TextGrid files. Define the path to this database in R and check that you can see these files with the list.files() function:\n\npath.praat &lt;- file.path(sourceDir, \"praat\")\nlist.files(path.praat)\n\n [1] \"wetter1.TextGrid\"  \"wetter1.wav\"       \"wetter10.TextGrid\"\n [4] \"wetter10.wav\"      \"wetter11.TextGrid\" \"wetter11.wav\"     \n [7] \"wetter12.TextGrid\" \"wetter12.wav\"      \"wetter13.TextGrid\"\n[10] \"wetter13.wav\"      \"wetter14.TextGrid\" \"wetter14.wav\"     \n[13] \"wetter15.TextGrid\" \"wetter15.wav\"      \"wetter16.TextGrid\"\n[16] \"wetter16.wav\"      \"wetter17.TextGrid\" \"wetter17.wav\"     \n[19] \"wetter2.TextGrid\"  \"wetter2.wav\"       \"wetter3.TextGrid\" \n[22] \"wetter3.wav\"       \"wetter4.TextGrid\"  \"wetter4.wav\"      \n[25] \"wetter6.TextGrid\"  \"wetter6.wav\"       \"wetter7.TextGrid\" \n[28] \"wetter7.wav\"      \n\n\nThe emuR function for converting the collection of .wav / .TextGrid pairs to an Emu database called and then storing the latter in targetDir (defined above) is convert_TextGridCollection(). It works like this:\n\nconvert_TextGridCollection(path.praat, \n                           dbName = \"praat\",\n                           targetDir = targetDir,\n                           verbose = FALSE)\n\nThe converted Praat database called praat_emuDB can now be loaded with load_emuDB():\n\npraat_DB &lt;- load_emuDB(file.path(targetDir, \"praat_emuDB\"),\n                       verbose = FALSE)\n\nAnd its properties can be examined with the summary() command as we’ve seen in previous chapters.\n\nsummary(praat_DB)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    praat \nUUID:    4c19bd0b-06eb-4500-b7ad-833ffceba0c9 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\praat_emuDB \nSession count: 1 \nBundle count: 14 \nAnnotation item count:  214 \nLabel count:  214 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name type    nrOfAttrDefs attrDefNames\n ORT  SEGMENT 1            ORT;        \n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nYou’ll see that there are no Link definitions, as .TextGrids do not offer this kind of structure.\nThe database can of course be viewed using the serve() command as we’ve seen in previous chapters.\n\nserve(praat_DB, useViewer = FALSE)"
  },
  {
    "objectID": "praat2emu.html#calculating-pitch-with-wrassp",
    "href": "praat2emu.html#calculating-pitch-with-wrassp",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.3 Calculating pitch with wrassp",
    "text": "4.3 Calculating pitch with wrassp\n\nThe summary() above showed that there are no SSFF tracks associated with the database – this is also not a file structure that Praat understands. Recall from Chapter 2 that SSFF tracks store signal data that is timelocked to the audio signal. In this next step, we will generate SSFF tracks for praat_emuDB containing information about pitch.\nIn order to do this, we use the signal processing package wrassp. There are several different signal processing packages in R, and several of them have pitch tracking algorithms. We work with wrassp here because it was developed in conjunction with emuR, and thus is very easy to integrate with emuR.\nTo see the full range of signal processing routines available in wrassp, enter ?wrassp in the R console. There are two functions for calculating pitch: ksvF0() and mhsF0(). For this demonstration, we will use mhsF0().\nThe function mhsF0() (and other signal processing functions in wrassp) operates directly on one or more .wav files, and creates corresponding new files containing pitch tracks. It does not care whether those .wav files are part of an Emu database, so to speak. In order to add SSFF tracks that are based on the signal processing capabilities in wrassp directly to an Emu database, we can use the handy emuR function add_ssffTrackDefinition() with the argument onTheFlyFunctionName. The onTheFlyFunctionName should usually be one of the signal processing routines in wrassp, although other functions can also be used.\nHere’s how to use mhsF0() to generate pitch tracks with the default settings and add them to a Emu database:\n\nadd_ssffTrackDefinition(praat_DB, name='pitch', onTheFlyFunctionName='mhsF0',\n                        verbose = FALSE)\n\nIf we run list_ssffTrackDefinitions() we will now see that an SSFF track called pitch with the file extension .pit has been added:\n\nlist_ssffTrackDefinitions(praat_DB)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nChanging the pitch tracking parameters\n\n\n\nIt is often not a good idea to use the default parameters for pitch tracking. For example, mhsF0() and ksvF0() by default search for pitch in an extremely broad frequency range (between 50–600 Hz), and narrowing this down often gives better results. We can see all possible mhsF0() parameters using the args() function, which returns all arguments of a given function:\n\nargs(mhsF0)\n\nfunction (listOfFiles = NULL, optLogFilePath = NULL, beginTime = 0, \n    centerTime = FALSE, endTime = 0, windowShift = 5, gender = \"u\", \n    maxF = 600, minF = 50, minAmp = 50, minAC1 = 0.25, minRMS = 18, \n    maxZCR = 3000, minProb = 0.52, plainSpectrum = FALSE, toFile = TRUE, \n    explicitExt = NULL, outputDirectory = NULL, forceToLog = useWrasspLogger, \n    verbose = TRUE) \nNULL\n\n\nAll of these arguments can be changed. If you want to change the settings when adding an SSFF track, you can pass a named list of parameters to the argument onTheFlyParams in the add_ssffTrackDefinition() call. A named list looks e.g. like this:\n\npitch_params &lt;- list(maxF = 400, minF = 75)\n\nThe function call to add pitch tracks with these parameters looks like this. Since we already have a track called pitch with the file extension pit, we call this one pitch2 and use the file extension pit2.\n\nadd_ssffTrackDefinition(praat_DB, name='pitch2', \n                        onTheFlyFunctionName='mhsF0',\n                        onTheFlyParams=pitch_params,\n                        fileExtension='pit2',\n                        verbose=FALSE)"
  },
  {
    "objectID": "praat2emu.html#displaying-the-pitch-files-in-the-emu-webapp",
    "href": "praat2emu.html#displaying-the-pitch-files-in-the-emu-webapp",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.4 Displaying the pitch files in the Emu-webApp",
    "text": "4.4 Displaying the pitch files in the Emu-webApp\nAs seen in Chapter 2, we can use the function get_signalCanvasesOrder() to see which signals are currently displayed in the database:\n\nget_signalCanvasesOrder(praat_DB, perspectiveName = \"default\")\n\n[1] \"OSCI\" \"SPEC\"\n\n\nThis confirms that what is seen when viewing the database with the serve() function is the waveform (OSCI) and the spectrogram (SPEC). The pitch data created above now needs to be added using the function set_signalCanvasesOrder().\n\nset_signalCanvasesOrder(praat_DB, \n                        perspectiveName = \"default\",\n                        order = c(\"OSCI\", \"SPEC\", \"pitch\"))\n\n(Alternatively, if you have installed emuhelpeR, you can add it like so: praat_DB %&gt;% add_signal_canvas('pitch')).\nThe pitch track should now be visible when you serve() the database:\n\nserve(praat_DB, useViewer = FALSE)"
  },
  {
    "objectID": "praat2emu.html#adding-an-event-level",
    "href": "praat2emu.html#adding-an-event-level",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.5 Adding an event level",
    "text": "4.5 Adding an event level\nThe next task is to add an annotation level of type EVENT (similar to “point tiers” in Praat) that can be used for labeling tones. We will call this level Tone. So far, the only existing annotation level with time information is ORT as confirmed with:\n\nlist_levelDefinitions(praat_DB)\n\n\n\n  \n\n\n\nIn order to add a new EVENT-type level called Tone, we use the function add_levelDefinition() like so:\n\nadd_levelDefinition(praat_DB, \n                    name = \"Tone\", \n                    type = \"EVENT\",\n                    verbose = FALSE)\n\nset_levelCanvasesOrder() can be used to ensure that Tone is shown when directly underneath the signals when serve()ing the database:\n\nset_levelCanvasesOrder(praat_DB, \n                       perspectiveName = \"default\", \n                       order = c(\"Tone\", \"ORT\"))"
  },
  {
    "objectID": "praat2emu.html#labelling-some-tones",
    "href": "praat2emu.html#labelling-some-tones",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.6 Labelling some tones",
    "text": "4.6 Labelling some tones\nTry and add two tone labels H* at the time of the pitch peaks of morgens and ruhig in the bundle wetter1 as in Figure 4.1 and save the result. The pointers for annotating given in Chapter 2 also apply here.\n\nserve(praat_DB, useViewer = FALSE)\n\nAt present, there are no defined relationships between the different annotation levels, as confirmed with list_linkDefinitions():\n\nlist_linkDefinitions(praat_DB)\n\nNULL\n\n\nWe would like the tones we annotate to be linked to words within which they occur in time. To do this, we use add_linkDefinition() to define a hierarchical relationship such that ORT dominates Tone:\n\nadd_linkDefinition(praat_DB, \n                   type = \"ONE_TO_MANY\", \n                   superlevelName = \"ORT\", \n                   sublevelName = \"Tone\")\n\nlist_linkDefinitions(praat_DB)\n\n\n\n  \n\n\n\nTo see the results of this procedure, serve() the database again and switch to hierarchy view.\n\nserve(praat_DB, useViewer = FALSE)"
  },
  {
    "objectID": "praat2emu.html#automatically-linking-event-and-segment-times",
    "href": "praat2emu.html#automatically-linking-event-and-segment-times",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.7 Automatically linking event and segment times",
    "text": "4.7 Automatically linking event and segment times\nOnce a hierarchical relationship has been established, between two levels, times can be linked using the autobuild_linkFromTimes() function, like so:\n\nautobuild_linkFromTimes(praat_DB,\n                        superlevelName = \"ORT\",\n                        sublevelName = \"Tone\",\n                        verbose = FALSE)\n\nTo see the results of this procedure, serve() the database again and switch to hierarchy view.\n\nserve(praat_DB, useViewer = FALSE)\n\nIn this chapter, we have seen how to convert a collection of Praat-based annotations and sound files into an Emu database, and we’ve taken the first steps towards analyzing and annotating pitch. In Chapter 6, we’ll go into more detail about the analysis and annotation of intonation.\n\n\n\n\n\n\nWant to make plots in the style of Praat Picture?\n\n\n\nA very nice feature of Praat is that when you’ve found a nice portion of the signal, it’s fairly simple to make a plot that combines waveform, spectrogram, other derived tracks, and annotations. Say you’ve fallen in love with the portion in wetter3 in our praat_emuDB, and you want to plot the zu Samstag portion. Unfortunately, there are no functions internal to emuR that allows you to plot audio data along with annotations, but the package praatpicture allows this.\npraatpicture can be installed in the usual way:\n\ninstall.packages(\"praatpicture\")\n\nThe function emupicture() can now be called to produce a Praat-style plot like so, setting the start and end time to plot our favorite part of the signal. The argument tg_tiers specifies which annotation levels (equivalent to TextGrid tiers should be plotted), and the wave_channels argument specifies which channels of the sound file should be plotted. (This is not visible in the Emu-webApp, but the sound files in praat_DB actually have two channels).\n\nlibrary(praatpicture)\nemupicture(praat_DB, bundle='wetter3', start=0.6, end=1.275, tg_tiers='ORT',\n           wave_channels=1)\n\n\n\n\nFor my information about the options you get with emupicture() and the praatpicture library, see the manual."
  },
  {
    "objectID": "praat2emu.html#functions-introduced-in-this-chapter",
    "href": "praat2emu.html#functions-introduced-in-this-chapter",
    "title": "4  Converting a Praat TextGrid collection and tracking pitch",
    "section": "4.8 Functions introduced in this chapter",
    "text": "4.8 Functions introduced in this chapter\n\n\nconvert_TextGridCollection(): converts pairs of .wav and .TextGrid files to an Emu database.\nmhsF0() and ksvF0(): generates files with pitch track information from one or more sound files.\nadd_ssffTrackDefinition(): adds SSFF files with track information timelocked with the audio signal; often done on the fly with signal processing functions from wrassp\nadd_linkDefinition(): establishes a hierarchical relationship between two annotation levels\nautobuild_linkFromTimes(): uses time information to link annotation items on two tiers automatically\nemupicture(): makes plots combining signal and annotation in the style of Praat Picture (requires praatpicture to be installed)"
  },
  {
    "objectID": "forced_alignment.html#objective-and-preliminaries",
    "href": "forced_alignment.html#objective-and-preliminaries",
    "title": "5  Forced alignment in emuR",
    "section": "5.1 Objective and preliminaries",
    "text": "5.1 Objective and preliminaries\nThe objective of this chapter is to show how to go from a directory with .wav files that are orthographically transcribed in simple .txt files to a phonetically annotated and force-aligned Emu database.\nThe assumption is that you already have an R project called ipsR and that it contains the directories emu_databases and testsample. If this is not the case, please go back and follow the preliminaries chapter.\n\nStart up R in the project you are using for this tutorial and load the usual packages:\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\n\nIn R, store the path to the directory testsample as sourceDir in the following way:\n\nsourceDir &lt;- \"./testsample\"\n\nAnd also store in R the path to emu_databases as targetDir:\n\ntargetDir &lt;- \"./emu_databases\""
  },
  {
    "objectID": "forced_alignment.html#converting-a-text-collection-into-an-emu-database",
    "href": "forced_alignment.html#converting-a-text-collection-into-an-emu-database",
    "title": "5  Forced alignment in emuR",
    "section": "5.2 Converting a text collection into an Emu database",
    "text": "5.2 Converting a text collection into an Emu database\nThe directory ./testsample/german on your computer contains .wav files and .txt files. Define the path to this directory in R and check that you can see these files with the list.files() function:\n\npath.german &lt;- file.path(sourceDir, \"german\")\nlist.files(path.german)\n\n[1] \"K01BE001.txt\" \"K01BE001.wav\" \"K01BE002.txt\" \"K01BE002.wav\"\n\n\nThe above is an example of what we’ll call a text collection because it contains matching .wav and .txt files in the same directory, and each .wav file, the .txt file contains the corresponding orthography. We can see that this is true by using the function read_file() to read the content of these .txt files:\n\nread_file(file.path(path.german, 'K01BE001.txt'))\n\n[1] \"heute ist schönes Frühlingswetter\"\n\nread_file(file.path(path.german, 'K01BE002.txt'))\n\n[1] \"die Sonne lacht\"\n\n\nThe command convert_txtCollection() is used to convert a text collection into an Emu database. Below we make an Emu database called ger2, which we’ll store in targetDir:\n\nconvert_txtCollection(dbName = \"ger2\",\n                      sourceDir = path.german,\n                      targetDir = targetDir,\n                      verbose=FALSE)\n\nYou can now load the database into R with load_emuDB():\n\nger2_DB &lt;- load_emuDB(file.path(targetDir, \"ger2_emuDB\"), verbose=FALSE)\nsummary(ger2_DB)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ger2 \nUUID:    6fa81b59-fc09-4ba8-8792-9a87a6062ad8 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\ger2_emuDB \nSession count: 1 \nBundle count: 2 \nAnnotation item count:  2 \nLabel count:  4 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type nrOfAttrDefs attrDefNames          \n bundle ITEM 2            bundle; transcription;\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nTry to serve() the database and have a look at it:\n\nserve(ger2_DB, useViewer = F)\n\nIf you switch to hierarchy view, you should see that the words in the .txt files are a single item in the attribute level of bundle with the name transcription, as shown in the image below:\n\nIf we query() the database to find all annotations at the transcription level, it’s clear that the words are all stored in this way. Note that we need to include the argument calcTimes = FALSE here, because the annotation level transcription is of type ITEM and has no time information.\n\nquery(ger2_DB, \"transcription =~ .*\", calcTimes = FALSE)"
  },
  {
    "objectID": "forced_alignment.html#forced-alignment",
    "href": "forced_alignment.html#forced-alignment",
    "title": "5  Forced alignment in emuR",
    "section": "5.3 Forced alignment",
    "text": "5.3 Forced alignment\nWe are now going to run the Munich Automatic Segmentation (MAUS) pipeline over the database. We do this with the function runBASwebservice_all(), which combines a number of online processing tools. The obligatory arguments of this function are transcriptionAttributeDefinitionName() which will be the name of the newly created annotation level, and language, which in this case we set to deu-DE. We also set the argument runMINNI to FALSE; this is potentially used to forced-align data which has no annotations at all by first running the MINNI automatic speech recognition (ASR) routine. Note that runBASwebservice_all() can only be used if you have an active internet connection.\n\nrunBASwebservice_all(ger2_DB,\n  transcriptionAttributeDefinitionName = \"transcription\",\n  language = \"deu-DE\", \n  runMINNI = FALSE,\n  verbose = FALSE)\n\n\n\n\n\n\n\nThe language setting in MAUS\n\n\n\nThere are quite a lot of languages available in MAUS. As of this writing, you can force-align Afrikaans, Albanian, Arabic, Basque, Catalan, Dutch, English, Estonian, Finnish, French, Georgian, German, Hungarian, Icelandic, Italian, Japanese, Luxembourgish, Maltese, Min Nan, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Swedish, and Thai. There’s also a language independent mode which expects files to be phonetically transcribed in X-SAMPA, and a special mode for Australian aboriginal languages. Additionally, many of these languages have modes for multiple different dialects. More information can be found in the MAUS help files and in the MAUS web interface.\n\n\nLet’s have a look at the database summary again:\n\nsummary(ger2_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ger2 \nUUID:    6fa81b59-fc09-4ba8-8792-9a87a6062ad8 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\ger2_emuDB \nSession count: 1 \nBundle count: 2 \nAnnotation item count:  58 \nLabel count:  74 \nLink count:  52 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type    nrOfAttrDefs attrDefNames          \n bundle ITEM    2            bundle; transcription;\n ORT    ITEM    3            ORT; KAN; KAS;        \n MAU    SEGMENT 1            MAU;                  \n MAS    ITEM    1            MAS;                  \n\n\n── Link definitions ──\n\n\n type        superlevelName sublevelName\n ONE_TO_MANY bundle         ORT         \n ONE_TO_MANY ORT            MAS         \n ONE_TO_MANY MAS            MAU         \n\n\nNote that multiple extra annotation levels and attributes have been created (ORT, KAN, KAS, MAU, and MAS), as well as links between them.\nLet’s serve() the database.\n\nserve(ger2_DB, useViewer = FALSE)\n\nWe immediately see that phone-level annotations have been added in the MAU level. Have a look at the hierarchy view and try to identify the levels, links, and attributes.\n\nThis shows that the phone-level annotations are linked to syllable-level annotations in the MAS level and word-level orthographic annotations in the ORT level. The ORT level further has the attributes KAN and KAS. These contain canonical representations of the word, i.e. phonetic annotations corresponding to the canonical pronunciations of these words. The MAS level is chunked into syllables.\nGiven this complex information, more complex queries are now also possible. We can find the word-initial MAU segments of all polysyllabic words like this:\n\nmau.s &lt;- query(ger2_DB,\n               \"[[MAU =~.* & Start(ORT, MAU)=1] ^ Num(ORT, MAS) &gt; 1]\")\n#deselect the db_uuid column for readability\nmau.s %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThis data frame can then be passed to requery_hier() so we can see the labels in the ORT level associated with these words, like so:\n\nrequery_hier(ger2_DB, mau.s, \"ORT\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThis may all seem rather opaque, but we’ll go into much more detail with how the querying language works in Chapter 7."
  },
  {
    "objectID": "forced_alignment.html#forced-alignment-albanian",
    "href": "forced_alignment.html#forced-alignment-albanian",
    "title": "5  Forced alignment in emuR",
    "section": "5.4 Forced alignment: Albanian",
    "text": "5.4 Forced alignment: Albanian\nNext we’ll try out forced alignment for a different language (Albanian) and we will show how forced alignment can be done from a canonical phonemic transcription instead of from text.\n\n5.4.1 From a text collection\nFirst we’ll use a text collection like we saw for German previously. This text collection is in our sourceDir in a folder called albanian:\n\npath.albanian &lt;- file.path(sourceDir, \"albanian\")\n\nFirst we’ll convert the text collection into an Emu database using convert_txtCollection() as above.\n\nconvert_txtCollection(dbName = \"alb\",\n                      sourceDir = path.albanian,\n                      targetDir = targetDir,\n                      verbose=FALSE)\n\nalb_DB &lt;- load_emuDB(file.path(targetDir, \"alb_emuDB\"), verbose = FALSE)\nsummary(alb_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    alb \nUUID:    ad493e68-da1c-438f-b042-3e1ea02e814e \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\alb_emuDB \nSession count: 1 \nBundle count: 4 \nAnnotation item count:  4 \nLabel count:  8 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type nrOfAttrDefs attrDefNames          \n bundle ITEM 2            bundle; transcription;\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nHave a look at the database, switch to hierarchy view, and verify that the words have been located at bundle -&gt; transcription as for the German database above.\n\nserve(alb_DB, useViewer = F)\n\nNow run MAUS, just as before. The language code for Albanian is sqi-AL. Note that this will take longer than for German, possibly a couple of minutes.\n\nrunBASwebservice_all(alb_DB, \n                     transcriptionAttributeDefinitionName = \"transcription\",\n                     language = \"sqi-AL\", \n                     runMINNI = FALSE,\n                     verbose = FALSE)\n\nsummary(alb_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    alb \nUUID:    ad493e68-da1c-438f-b042-3e1ea02e814e \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\alb_emuDB \nSession count: 1 \nBundle count: 4 \nAnnotation item count:  138 \nLabel count:  176 \nLink count:  125 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type    nrOfAttrDefs attrDefNames          \n bundle ITEM    2            bundle; transcription;\n ORT    ITEM    3            ORT; KAN; KAS;        \n MAU    SEGMENT 1            MAU;                  \n MAS    ITEM    1            MAS;                  \n\n\n── Link definitions ──\n\n\n type        superlevelName sublevelName\n ONE_TO_MANY bundle         ORT         \n ONE_TO_MANY ORT            MAS         \n ONE_TO_MANY MAS            MAU         \n\n\nLook at the database and verify that the same kind of information has been automatically derived, as for the German database earlier.\n\nserve(alb_DB, useViewer = FALSE)\n\n\n\n5.4.2 From a canonical representation\nMAUS also allows to segment sound files based on the canonical level that we saw in the KAN attribute above. This can be useful when the canonical representation provided by MAUS deviates considerably from what was actually said. For one of the words in 0001BF_1syll_1, the canonical representation is J E but what was actually said is closer to n J E.\nTo see how this works, switch the hierarchy view in the webApp from ORT → KAN and then change the node J E of the ORT:KAN level to n J E for file 0001BF_1syll_1, as in ?fig-albhier. Remember to save the annotation after editing.\n\nWe can now run MAUSdirectly on this canonical level using the runBASwebservice_maus() function. Here we again pass the language, and we pass the name of the existing annotation level with a canonical representation KAN to the argument canoAttributeDefinitionName. We also specify the newly created force-aligned level mausAttributeDefinitionName, which we call MAU2 in order to differentiate it MAU level that we already created.\n\nrunBASwebservice_maus(alb_DB,\n                      canoAttributeDefinitionName = \"KAN\",\n                      mausAttributeDefinitionName = \"MAU2\",\n                      language = \"sqi-AL\",\n                      verbose = FALSE)\n\nInspect the database again. There should now be a new annotation level called MAU2.\n\nsummary(alb_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    alb \nUUID:    ad493e68-da1c-438f-b042-3e1ea02e814e \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\alb_emuDB \nSession count: 1 \nBundle count: 4 \nAnnotation item count:  219 \nLabel count:  257 \nLink count:  197 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type    nrOfAttrDefs attrDefNames          \n bundle ITEM    2            bundle; transcription;\n ORT    ITEM    3            ORT; KAN; KAS;        \n MAU    SEGMENT 1            MAU;                  \n MAS    ITEM    1            MAS;                  \n MAU2   SEGMENT 1            MAU2;                 \n\n\n── Link definitions ──\n\n\n type        superlevelName sublevelName\n ONE_TO_MANY bundle         ORT         \n ONE_TO_MANY ORT            MAS         \n ONE_TO_MANY MAS            MAU         \n ONE_TO_MANY ORT            MAU2        \n\n\nIf you have reason to suspect that the canonical representation will differ from what is actually said in the recording, you can use the function runBASwebservice_g2pForPronunciation() as the first step. This will only generate the canonical reprsentations without doing forced alignment, and this can then be corrected before running the MAUS forced alignment routine. If you have much more data, this may speed up your process."
  },
  {
    "objectID": "forced_alignment.html#functions-introduced-in-this-chapter",
    "href": "forced_alignment.html#functions-introduced-in-this-chapter",
    "title": "5  Forced alignment in emuR",
    "section": "5.5 Functions introduced in this chapter",
    "text": "5.5 Functions introduced in this chapter\n\nread_file(): reads the content of a .txt file into R\nconvert_txtCollection(): converts pairs of .wav and .txt files into an Emu database.\nrun_BASwebservice_all(): performs all the steps needed to get from an Emu database with orthographical transcription that aren’t time-aligned to a multi-level force-aligned phonetic annotation\nrun_BASwebservice_maus(): performs forced alignment on an Emu database which already has (non-time-aligned) canonical phonetic annotations\nrun_BASwebservice_g2pForPronunciation(): performs grapheme-to-phoneme (G2P) conversion of an orthographical transcription to a canonical phonetic transcription on an Emu database"
  },
  {
    "objectID": "tobi2emu.html#objective-and-preliminaries",
    "href": "tobi2emu.html#objective-and-preliminaries",
    "title": "6  Configuring emuR for ToBI annotations",
    "section": "6.1 Objective and preliminaries",
    "text": "6.1 Objective and preliminaries\nThe objective of this chapter is to show how to go from a collection of .wav and .txt files to an Emu database that is set up for tones-and-break indices (ToBI) annotation. More specifically, the aim is to configure a database to have a configuration like that shown in Figure 6.1.\n\n\n\nFigure 6.1: An Emu database with ToBI annotations.\n\n\nThe starting point is the text collection of Albanian data analyzed in Chapter 5.\nWe will also see how an annotation level with syllable structures can be ascertained from a database by creating a CV-style annotation level.\nWe assume that you already have an R project called ipsR and that it contains the directories emu_databases and testsample. If this is not the case, please go back and follow the preliminaries chapter. As in previous chapters, we start by loading the usual libraries and renaming the directories to sourceDir and targetDir.\n\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\nsourceDir &lt;- \"./testsample\"\ntargetDir &lt;- \"./emu_databases\""
  },
  {
    "objectID": "tobi2emu.html#forced-alignment",
    "href": "tobi2emu.html#forced-alignment",
    "title": "6  Configuring emuR for ToBI annotations",
    "section": "6.2 Forced alignment",
    "text": "6.2 Forced alignment\nThe first task is to force-align these Albanian data. The commands will be presented in this section without detailed comment since these were already discussed in Chapter 5. The resulting Emu database will be stored as alb2_DB (to distinguish it from the one created in the Chapter 5).\nWe first create and load the Emu database using convert_txtCollection() and load_emuDB(), and subsequently use runBASwebservice_all() to run the MAUS pipeline over the data.\n\npath.albanian &lt;- file.path(sourceDir, \"albanian\")\nconvert_txtCollection(dbName = \"alb2\", \n                      sourceDir = path.albanian,\n                      targetDir = targetDir,\n                      verbose = FALSE)\nalb2_DB &lt;- load_emuDB(file.path(targetDir, \"alb2_emuDB\"),\n                      verbose = FALSE)\nrunBASwebservice_all(alb2_DB,\n                     transcriptionAttributeDefinitionName = \"transcription\", \n                     language = \"sqi-AL\",  \n                     runMINNI = FALSE,\n                     verbose = FALSE)"
  },
  {
    "objectID": "tobi2emu.html#configuring-a-database-for-tobi-annotations",
    "href": "tobi2emu.html#configuring-a-database-for-tobi-annotations",
    "title": "6  Configuring emuR for ToBI annotations",
    "section": "6.3 Configuring a database for ToBI annotations",
    "text": "6.3 Configuring a database for ToBI annotations\nThe database currently has an ITEM-type annotation level ORT that dominates an ITEM-type level MAS containing syllabifications. MAS dominates the SEGMENT-type level MAU, which contains the phonetic segmentation. You can confirm this either by serve()ing the database or by calling summary(alb2_DB).\nOur next task is to add a level Tone for marking intonational events and to add a new SEGMENT level, ORT2 which contains word-level segmentations. The tonal events of the Tone level and annotations of the ORT2 level are to be linked automatically, just as in Figure 6.1.\nThe steps required to get there are as follows:\n\nCreate a SEGMENT-type level called ORT2 containing word annotations linked to times.\nCreate a Tone level and link it to ORT2.\nCalculate and display pitch tracks.\nAnnotate the Tone level and link the annotations automatically to the word level annotations in ORT2.\n\n\n6.3.1 Create and display word level annotations\nFirst we create and display word level annotations. This is a four step process:\nFirst, create a SEGMENT-type annotation level called ORT2.\nAs covered in Chapter 4, this can be done with the add_levelDefinition() function like so:\n\nadd_levelDefinition(alb2_DB, \n                    name = \"ORT2\",\n                    type = \"SEGMENT\",\n                    verbose = FALSE)\n\nWe can confirm that this step was successful with the function list_levelDefinitions():\n\nlist_levelDefinitions(alb2_DB)\n\n\n\n  \n\n\n\nSecond, make a list of all words in the database.\nWe can do this by using the query() function to find all annotations in the ORT level (as in previous chapters, we’ll also remove the db_uuid column from the printed results because the data frame is more legible this way).\n\ntext.s &lt;- query(alb2_DB, \"ORT =~ .*\")\ntext.s %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nWe can use the segment list generated by the query() above to populate our new level ORT2. We simply need to change the columns attribute and level so they show the value ORT2 instead of ORT:\n\ntext.s$attribute &lt;- \"ORT2\"\ntext.s$level &lt;- \"ORT2\"\n\nThird, add word-level annotations to ORT2.\nWe can now pass text.s on to the function create_itemsInLevel() like so:\n\ncreate_itemsInLevel(alb2_DB, text.s,\n                    verbose = FALSE,\n                    calculateEndTimeForSegments = FALSE)\n\nIt is now possible to query() the word level annotations directly from ORT2:\n\nquery(alb2_DB, \"ORT2 =~ .*\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nFourth, display the annotations of ORT2 time-aligned to the signals.\nBecause ORT2 is a SEGMENT-type level and not an ITEM-type level, we can now display word-level annotations underneath the signals in the EMU-webApp. However, if we serve() the database now, only the segment-level annotations in MAU can be seen. As covered in Chapter 4, this can be controlled with the set_levelCanvasesOrder() function like so:\n\nset_levelCanvasesOrder(alb2_DB,\n                       perspectiveName = \"default\",\n                       order = \"ORT2\")\n\nserve()ing the database now will show only the word-level annotations in ORT2 underneath the signals:\n\nserve(alb2_DB, useViewer = FALSE)\n\n\n\n6.3.2 Create a Tone level and link it to ORT2\nCreating new annotation level called Tone and linking it to ORT2 is a three-step process:\nFirst, create the new annotation level named Tone.\nWe do this with the add_levelDefinition() function, just as we did earlier when creating ORT2. Tone should be an EVENT-type level, since we want to mark intonational targets as single points in time.\n\nadd_levelDefinition(alb2_DB, \n                    name = \"Tone\", \n                    type = \"EVENT\",\n                    verbose = FALSE)\n\nSecond, create the link between Tone and ORT2.\nThe function list_linkDefinitions() shows that there are no currently no links between Tone and ORT2:\n\nlist_linkDefinitions(alb2_DB)\n\n\n\n  \n\n\n\nIn order to allow for annotations in the two levels to be queried relative to each other, they need to be linked. On the assumption that a word can be associated with one or more tones, but that a given tone can only be associated with one word, the association between the two levels should be ONE_TO_MANY. The levels should therefore be linked in the following way with the add_linkDefinitions() function:\n\nadd_linkDefinition(alb2_DB, \n                   type = \"ONE_TO_MANY\", \n                   superlevelName = \"ORT2\", \n                   sublevelName = \"Tone\")\n\nCalling list_linkDefinitions() again will confirm that the two levels are now linked:\n\nlist_linkDefinitions(alb2_DB)\n\n\n\n  \n\n\n\nThird, display the Tone level.\nAs before, this can be done with the set_levelCanvasesOrder() function. We now need to specify both Tone and ORT2 in the order argument, like so:\n\nset_levelCanvasesOrder(alb2_DB,\n                       perspectiveName = \"default\",\n                       order = c(\"Tone\", \"ORT2\"))\n\nserve()ing the database again will now show both the Tone and ORT2 levels underneath the signals:\n\nserve(alb2_DB, useViewer = FALSE)\n\n\n\n6.3.3 Calculating and displaying F0\nCalculating F0 and displaying it in the webApp is a three-step process. Most of these steps were covered in Chapter 4, so only a brief summary is given here.\nFirst, calculate F0.\nWe can do this using the function add_ssffTrackDefinition with the ‘on the fly function’ mhsF0() (see Chapter 4).\n\nadd_ssffTrackDefinition(alb2_DB, \n                        name = \"pitch\", \n                        onTheFlyFunctionName = \"mhsF0\",\n                        verbose = FALSE)\n\nWe can confirm that this worked with the function list_ssffTrackDefinitions():\n\nlist_ssffTrackDefinitions(alb2_DB)\n\n\n\n  \n\n\n\nSecond, configure the database to show the F0 track.\nAs explained in Chapter 4, the pitch data can be displayed with the function set_signalCanvasesOrder(). Currently, only the waveform and spectrogram are displayed, as confirmed with the get_signalCanvasesOrder() function:\n\nget_signalCanvasesOrder(alb2_DB, perspectiveName = \"default\")\n\n[1] \"OSCI\" \"SPEC\"\n\n\nIn order to display the pitch data underneath the spectrogram, use set_signalCanvasesOrder() like follows:\n\nset_signalCanvasesOrder(alb2_DB, \n                        perspectiveName = \"default\",\n                        order = c(\"OSCI\", \"SPEC\", \"pitch\"))\n\n(Alternatively, if you have installed and loaded emuhelpeR, you can display it like so: alb2DB %&gt;% add_signal_canvas(\"pitch\").)\n\n\n\n\n\n\nOverlaying pitch on the spectrogram\n\n\n\nIf you have the library emuhelpeR installed, you can overlay pitch on the spectrogram using the signal_on_spec() function like so:\n\nlibrary(emuhelpeR)\nalb2_DB %&gt;% signal_on_spec(\"pitch\")\n\n(The function clear_spec() from emuhelpeR can be used to remove the signal from the spectrogram if you no longer wish to display it there.)\nIf you do not have emuhelpeR installed, the process is a bit more intricate. For this you will usually use a plain text editor to change the database’s configuration JSON file. This file always has the same name as the database + config.json and is stored in the same directory as the database. Once you’ve found this file, you can proceed as follows:\n\nOptionally make a backup copy of the the alb2_DBconfig.json file in case anything goes wrong.\nOpen alb2_DBconfig.json with a plain text editor, e.g. Notepad in Windows.\nSearch the file for the text string assign. You should find a line with the following contents:\n\n\"assign\": [],\n\nCarefully replace this line with the following:\n\n\"assign\": [{ \"signalCanvasName\": \"SPEC\", \"ssffTrackName\": \"pitch\" }],\n\nSave the file.\nserve() the database again.\n\nIn principle, the configuration JSON file can also be edited in R, e.g. using the rjson library. You should probably avoid this unless you’re an advanced R user – it’s a finnicky process and a lot can go wrong.\n\n\n\n\n6.3.4 Automatically linking annotations at the Tone level\nThe first task here is to make a few annotations. More specifically, add two pitch targets (e.g., L* to Lena, L* to leu) to the utterance 0001BF_1syll_1 as in Figure 6.1. See Chapter 3 for details of how to annotate in the Emu-webApp.\nThe next task is to automatically link the annotations at the Tone level to the corresponding words in the ORT2 level. We can do this using the function autobuild_linkFromTimes(). This function links an annotation at the Tone level to an annotation at the ORT2 level if the Tone annotation falls within the segment segment boundaries of the ORT2 annotation. In other words, if the start and end times of an ORT2 annotation are denoted t1 and t2, and the time of a Tone annotation is denoted simply t, then annotations at the two levels are linked if t1 &lt; t &lt; t2.\n\nautobuild_linkFromTimes(alb2_DB,\n                        superlevelName = \"ORT2\",\n                        sublevelName = \"Tone\",\n                        verbose = FALSE)\n\nIf you serve() the database again and look at the hierarchy view, you can confirm that the links were indeed created.\nIt should now be possible to query() tones with respect to words and vice-versa. For example, the below query finds all words at the ORT2 level with an annotated pitch accent at the Tone level. (Much more detail on querying follows in Chapter 7!)\n\nquery(alb2_DB, \"[Tone =~.* ^ #ORT2 =~.*]\") %&gt;% select(-db_uuid)"
  },
  {
    "objectID": "tobi2emu.html#adding-a-cv-level",
    "href": "tobi2emu.html#adding-a-cv-level",
    "title": "6  Configuring emuR for ToBI annotations",
    "section": "6.4 Adding a CV level",
    "text": "6.4 Adding a CV level\nIt can be helpful to have another annotation level in which every vowel corresponds to V and every consonant to C. This could then be used to establish the syllable types in the database. To do this, a new level called CV will be created that is an attribute of the MAU level (which contains the segment annotations). We do this with the add_attributeDefinition() function:\n\nadd_attributeDefinition(alb2_DB,\n                        levelName = \"MAU\",\n                        name = \"CV\",\n                        verbose=FALSE)\n\nHere we query all segments found at the MAU level in the database:\n\nmau.s &lt;- query(alb2_DB, \"MAU =~ .*\")\nmau.s %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nWe can find unique segments (and their frequencies) using the count function available in the tidyverse:\n\ncount(mau.s, labels)\n\n\n\n  \n\n\n\nWe’ll store the vowels in a vector vowel_phonemes:\n\nvowel_phonemes = c(\"a\", \"E\", \"i\", \"O\", \"u\", \"4\")\n\nWith a few tidyverse commands, we can wrangle mau.s to have the label V for vowels and C for consonants (i.e., the labels that are not in the vowel_phonemes vector), and store this as a new data frame called new_items.\nWe first change the values in the attribute column to CV using the mutate() function. We use the rename() function to change the labels column with phonemic annotations to old_labels. Finally we use mutate() in combination with case_when() to store the label V in labels wherever old_labels has a phonemic annotation found in our vowel_phonemes vector, and to store the label C in all other cases. For more resources on the tidyverse see the initial setup chapter.\n\n\nnew_items &lt;- mau.s %&gt;%\n  mutate(attribute = \"CV\") %&gt;%\n  rename(old_labels = labels) %&gt;%\n  mutate(labels = case_when(old_labels %in% vowel_phonemes ~ \"V\",\n                            TRUE ~ \"C\"))\n\nCalling the count() function on new_items will give us the number of consonants and vowels in the database:\n\ncount(new_items, labels)\n\n\n\n  \n\n\n\nWe can add these new annotations to the attribute level CV using the update_itemsInLevel() function like so:\n\nupdate_itemsInLevel(alb2_DB, \n                    itemsToUpdate = new_items,\n                    verbose = FALSE) \n\nWe can now query() the CV level for all of its annotations:\n\nseglist_cv &lt;- query(alb2_DB, \"CV =~ .*\")\ncount(seglist_cv, labels)\n\n\n\n  \n\n\n\nMore importantly, it is now possible to identify the different types of syllable structure. Recall that the MAS level parses the Phoneme level into syllables.\n\nsyll &lt;- query(alb2_DB, \"MAS =~ .*\")\ncount(syll, labels)\n\n\n\n  \n\n\n\nHere are the syllables in terms of their representation at the CV tier:\n\nsyll.cv &lt;- requery_hier(alb2_DB, syll, level = \"CV\")\ncount(syll.cv, labels)\n\n\n\n  \n\n\n\nDon’t worry if you don’t yet understand how the query()ing functions work – much more on that in Chapter 7!"
  },
  {
    "objectID": "tobi2emu.html#functions-introduced-in-this-chapter",
    "href": "tobi2emu.html#functions-introduced-in-this-chapter",
    "title": "6  Configuring emuR for ToBI annotations",
    "section": "6.5 Functions introduced in this chapter",
    "text": "6.5 Functions introduced in this chapter\n\nsignal_on_spec(): Overlays a signal, such as pitch, on top of the spectrogram in the Emu-webApp. (Requires emuhelpeR to be installed).\ncount(): Provides a count of all instances of a factor variable in a data frame or tibble.\nrequery_hier(): Requeries an emuDB based on an existing query. Will be discussed much more in Chapter 7."
  },
  {
    "objectID": "query.html#objective-and-preliminaries",
    "href": "query.html#objective-and-preliminaries",
    "title": "7  The Emu query language",
    "section": "7.1 Objective and preliminaries",
    "text": "7.1 Objective and preliminaries\nThe objective of this chapter is to show how to use the Emu query language for hierarchically structured annotations. The query language takes a bit of practice to get used to, but it’s a highly practical tool and making the effort is worth it!\nWe assume that you already have an R project called ipsR. If this is not the case, please go back and follow the preliminaries chapter.\n\nWe’ll load the usual libraries:\n\nlibrary(tidyverse)\nlibrary(emuR)\n\nIn this chapter we will make use of the demonstration database that we also saw in Chapter 2. We will download and store the demo database as we did in that chapter:\n\ncreate_emuRdemoData(dir = tempdir())\npath.ae &lt;- file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\nae &lt;- load_emuDB(path.ae, verbose = FALSE)\nsummary(ae)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ae \nUUID:    0fc618dc-8980-414d-8c7a-144a649ce199 \nDirectory:   C:\\Users\\rasmu\\AppData\\Local\\Temp\\Rtmpgxc4JW\\emuR_demoData\\ae_emuDB \nSession count: 1 \nBundle count: 7 \nAnnotation item count:  736 \nLabel count:  844 \nLink count:  785 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\n name columnName fileExtension fileFormat\n dft  dft        dft           ssff      \n fm   fm         fms           ssff      \n\n\n── Level definitions ──\n\n\n name         type    nrOfAttrDefs attrDefNames       \n Utterance    ITEM    1            Utterance;         \n Intonational ITEM    1            Intonational;      \n Intermediate ITEM    1            Intermediate;      \n Word         ITEM    3            Word; Accent; Text;\n Syllable     ITEM    1            Syllable;          \n Phoneme      ITEM    1            Phoneme;           \n Phonetic     SEGMENT 1            Phonetic;          \n Tone         EVENT   1            Tone;              \n Foot         ITEM    1            Foot;              \n\n\n── Link definitions ──\n\n\n type         superlevelName sublevelName\n ONE_TO_MANY  Utterance      Intonational\n ONE_TO_MANY  Intonational   Intermediate\n ONE_TO_MANY  Intermediate   Word        \n ONE_TO_MANY  Word           Syllable    \n ONE_TO_MANY  Syllable       Phoneme     \n MANY_TO_MANY Phoneme        Phonetic    \n ONE_TO_MANY  Syllable       Tone        \n ONE_TO_MANY  Intonational   Foot        \n ONE_TO_MANY  Foot           Syllable    \n\n\nAs seen in the summary(), the database has an EVENT-type level called Tone in which annotations are defined by single points in time, one SEGMENT-type level called Phonetic with start and end times, and several ITEM-type levels which inherit times from the Phonetic level, e.g. Syllable and Word. The link definitions summary shows a rich annotation structure that produces the tree-like structure seen in Figure 7.1 for the first utterance (note that only a single path through the hierarchy is shown). You should see the same if you serve() the database and go to the hierarchy view.\n\n\n\nFigure 7.1: Hierarchy of the first utterance of the database ae."
  },
  {
    "objectID": "query.html#first-steps-with-the-query-language",
    "href": "query.html#first-steps-with-the-query-language",
    "title": "7  The Emu query language",
    "section": "7.2 First steps with the query language",
    "text": "7.2 First steps with the query language\n\n7.2.1 A simple query\nThe function for computing queries is called query(). You will have seen this function in previous chapters, but this is where we describe how it works. query() needs at least two arguments: the name of the database and the query itself, e.g.\n\nV &lt;- query(ae, \"Phonetic == V\")\n\nThe expression [Phonetic == V] is a legal expression in the EMU Query Language (EQL) (more on this below) and can be translated into prose as: return those annotations in the Phonetic level that contain the label V and only the label V“ (V is the SAMPA for English equivalent to IPA /ʌ/, i.e. the vowel in words like cut).\n\n\n7.2.2 The object returned by query()\nquery() finds three instances of labels equaling V in the Phonetic level. Recall that we stored the output of our query() in the object V. We remove the db_uuid column from the resulting data frame here so the results are easier to read.\n\nV %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThis is an object of the type tibble, similar to a data frame, containing one row per segment. We’ll refer to the objects returned by query() as segment lists. Segment lists contain the following information:\n\nlabels contains annotations or sequenced annotations concatenated by -&gt;; more on this below\nstart contains start times in milliseconds\nend contains end times in milliseconds\ndb_uuid (removed above) is a unique identifier of the EMU database\nsession contains the session name where the segment is found\nbundle contains the file bundle name where the segment is found\nlevel contains the name of the annotation level which has been searched\nattribute contains the name of the attribute which has been searched\ntype contains the annotation level type, i.e. ITEM, EVENT, or SEGMENT (see Chapter 2)\nsample_start contains the position of the first sample in the sound file\nsample_end contains the position of the last sample in the sound file\nsample_rate contains the sample rate of the sound file (i.e. the start and end columns can be calculated by dividing sample_start or sample_end by sample_rate)\n\nThe tibble also contains the columns start_item_id, end_item_id, start_item_seq_idx, and end_item_seq_idx, which are used internally by emuR to query the JSON files where the annotations are stored.\nIt is straightforward to access information from the segment lists, by accessing these columns:\n\nV$labels; V$start; V$end\n\n[1] \"V\" \"V\" \"V\"\n\n\n[1]  187.425  340.175 1943.175\n\n\n[1]  256.925  426.675 2037.425\n\n\nAlternatively, using tidyverse-style functions:\n\nV %&gt;% pull(labels)\n\n[1] \"V\" \"V\" \"V\"\n\nV %&gt;% pull(start)\n\n[1]  187.425  340.175 1943.175\n\nV %&gt;% pull(end)\n\n[1]  256.925  426.675 2037.425\n\n\nIt also makes it easy to e.g. calculate the durations of all intervals:\n\nV$end - V$start\n\n[1] 69.50 86.50 94.25\n\n\nAlternatively in tidyverse style:\n\nV %&gt;% \n  mutate(dur = end - start) %&gt;% \n  pull(dur)\n\n[1] 69.50 86.50 94.25\n\n\nIn the above example, v is a segment list with start and end times for each item because Phonetic is a SEGMENT-type level. EVENT-type levels can be queried as well – in this case the structure of the tibble that is returned is exactly the same, except only 0s are returned in the end column, since the annotations mark unique events in time. For example, here is a ‘segment list’ of all tones showing just a few columns:\n\ntones &lt;- query(ae, \"Tone =~ .*\")\ntones %&gt;% select(labels, start, end, bundle)\n\n\n\n  \n\n\n\n\n\n7.2.3 Inherited times\nAnnotations in ITEM-type levels that either have no times or that inherit times from another level can be can queried in the same way. As the following shows, Phonetic is a SEGMENT-type level, and Phoneme is an ITEM-type annotation level without intrinsic time information.\n\nlist_levelDefinitions(ae)\n\n\n\n  \n\n\n\nIf we query() ae as above but searching the Phoneme level rather than the Phonetic level for V, we get the following results:\n\nV_phoneme &lt;- query(ae, \"Phoneme == V\")\nV_phoneme %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nDespite Phoneme being an ITEM-type level, the output has start and end times! In fact, they are identical to the start and end times of our previous query:\n\nV_phoneme$start; V$start\n\n[1]  187.425  340.175 1943.175\n\n\n[1]  187.425  340.175 1943.175\n\n\nThis is because times in the Phoneme level are in this case inherited from the Phonetic level (see Chapter 2 for a reminder why this is the case). The calculation of inherited times can be time-consuming in larger segment lists and it can be turned off with the calcTimes argument like so:\n\nV_phoneme2 &lt;- query(ae,\n                    \"[Phoneme == V]\",\n                    calcTimes = FALSE)\nV_phoneme2 %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nIn this case, all values in the start and end columns are returned as NA (= Not Available).\n\n\n7.2.4 Requerying\nThe function requery_hier() allows for segment lists (or ‘event lists’) to be created for an annotation level linked to a previously created segment list. In the above case, the annotation level of the segment list V_phoneme was Phoneme which is linked directly to the Syllable and Phonetic levels.\nSince the annotations are hierarchically structured (Syllable is linked to Word, Word is linked to the Intermediate level etc.) and Phoneme is near the bottom of this structure, this makes it possible to query just about any other level (see Figure 7.1). Using the function requery_hier(), we can use our existing segment list V_phoneme to query the annotations at another level during those same times. Below, we requery Text, which is an attribute of Word, to get the orthographic representation of words which appear in V_phoneme.\n\nt.s &lt;- requery_hier(ae, \n                    seglist = V_phoneme, \n                    level = \"Text\")\nt.s %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThe above is a requery looking “upstream” to a level that dominates Phoneme. A downstream query makes a segment list of all annotations that are found delimited by -&gt;. This means that we can use the segment list t.s that we just created to get a list of all anotations at the Phoneme level for those words:\n\nrequery_hier(ae, \n             seglist = t.s, \n             level = \"Phoneme\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nAs another example, we can use query() to make a segment list of all words, and requery() to find out which of these words are associated with tones. This is possible because the Word level (and therefore also its attribute Text) is linked to the Tone level via the Syllable level, as we saw above when calling list_linkDefinitions(ae) and in Figure 7.1 above.\n\nall.s &lt;- query(ae, \"Text =~ .*\")\nrequery_hier(ae, \n             seglist = all.s, \n             level = \"Tone\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nA lot of these rows are marked NA – this is because there is no annotation in the Tone level associated with that word, i.e. the word has no pitch accent.\nThe function requery_seq() is used for finding annotations that sequentially precede or follow those in an existing segment list. In contrast to the requery_hier() function, the segment lists returned from requery_seq() are always from the same annotation level as the segment list being requeried. The argument offset, which takes a positive or negative integer, finds following annotations if the integer is positive, and preceding annotations if the integer is negative. Thus to find the phonemes that immediately follow those of the segment list V_phoneme, we do the following:\n\nrequery_seq(emuDBhandle = ae,\n            seglist = V_phoneme,\n            offset = 1) %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nTo find the phonemes 2 positions prior to those in V_phoneme, the command should be the same but with offset = -2.\n\nrequery_seq(emuDBhandle = ae,\n            seglist = V_phoneme,\n            offset = -2)\n\nError in requery_seq(emuDBhandle = ae, seglist = V_phoneme, offset = -2): 1 of the requested sequence(s) is/are out of boundaries.\nSet parameter 'ignoreOutOfBounds=TRUE' to get residual result segments that lie within the bounds.\n\n\nThis command fails! This is because in some cases there are no annotations two positions prior to those in V_phoneme. As the error message tells us, we can get around this problem with the additional argument ignoreOutOfBounds. Let’s try that.\n\nrequery_seq(emuDBhandle = ae,\n            seglist = V_phoneme,\n            offset = -2,\n            ignoreOutOfBounds = TRUE) %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nThis returns NA in the first row, because there are no annotations two positions prior to this segment.\nA further variation on the requery_seq() function is to include the argument length, which takes a positive integer. This finds a sequence of annotations of the specified length at a given offset position. For example, the following makes a segment list that extends from 1–3 annotations to the right relative to the segment list V_phoneme:\n\nrequery_seq(emuDBhandle = ae,\n            seglist = V,\n            offset = 1, \n            length = 3) %&gt;% \n  select(-db_uuid)"
  },
  {
    "objectID": "query.html#more-complex-queries",
    "href": "query.html#more-complex-queries",
    "title": "7  The Emu query language",
    "section": "7.3 More complex queries",
    "text": "7.3 More complex queries\nSo far, we have only covered how query() can be used to find exact matches of a character string. The functionality of the EQL is a fair bit broader, and in the rest of this chapter, we give an overview of what it can do.\nquery() takes an argument also named query. This argument takes a string, meaning that the actual query must always be placed in quotation marks \" \". Any query can be placed within square brackets [ ], and as we will see below, this is sometimes necessary. The query must minimally include the name of an annotation level and a representation of an annotation – these can be a character or string of characters to search for in that annotation level, but can also be regular expression. More on this below.\n\n7.3.1 EQL operators\nIn the examples above, we found annotations in the Phonetic or Phoneme levels that were exactly equal to V with the following:\n\nquery(ae, \"Phonetic == V\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\n== is an operator indicating equality. For backwards compatibility with earlier versions of emuR, a single = is also allowed and gives the same result:\n\nquery(ae, \"Phonetic = V\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nSearches can also be made for everything except V by using the inequality operator != (“is not equal to”) as below:\n\nquery(ae, \"Phonetic != V\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThis returns every segment in the Phonetic level except those with V labels.\nFor finding matches that are not exact, e.g. for finding part of a string, the matching operator =~ is used. A special but very useful case for this operator is to find all annotations in an annotation level. This can be achieved with the combined regular expression .*. In regular expressions, . is a wildcard that can stand for any character, and * means that it can be repeated any number of times. The query Phonetic =~ .* thus means “return strings with any characters of any length from the Phonetic level”, i.e., “return everything from the Phonetic level”.\n\n\nquery(ae, \"Phonetic =~ .*\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThis fits, because our query() for exact matches of V had a 3 rows, our query() including everything but V had 250 rows, and our query for everything has 253 rows!\nWe could also use the matching operator for e.g. finding all annotations at the Text level that begin with a. We do this with the query Text =~ '^a.*', which searches for all words where the first character (this is indicated with ^) is a. (Note that any queries using the ^ operator requires the matching string to be enclosed with quotation marks ' ').\n\nquery(ae, \"Text =~ '^a'\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThere is a corresponding non-matching operator !~, which we could use for finding all words that don’t start with a using the same syntax:\n\nquery(ae, \"Text !~ '^a'\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThe | operator (the “or”-operator) can be used to search for several annotations in one go. For example, searching for instances of either m or n in the Phonetic level can be accomplished with the following:\n\nquery(ae, \"Phonetic == m | n\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\n| can be used with an arbitrary number of strings:\n\nquery(ae, \"Phonetic =~ ai | ei | oi\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\n\n\n7.3.2 Features\nIt is possible to define features and then query them. The definition of features is accomplished with the function add_attrDefLabelGroups(). For the existing ae database some features have already been defined. To see these for the Phonetic level, we can use the function list_attrDefLabelGroups():\n\nlist_attrDefLabelGroups(ae, \n                        levelName = \"Phonetic\",\n                        attributeDefinitionName = \"Phonetic\")\n\n\n\n  \n\n\n\nThis means that the strings found in the Phonetic level have already been classified according to some features. For example the feature nasal, which covers the annotations m and n. Consequently, the following two queries will give the same output:\n\nnas.s1 &lt;- query(ae, \"Phonetic == nasal\")\nnas.s2 &lt;- query(ae, \"Phonetic == m | n\")\n\nWe can check that this is true with the function call all(nas.s1 == nas.s2) which checks whether all cells in the two tibbles are identical.\n\nall(nas.s1 == nas.s2)\n\n[1] TRUE\n\n\nAs mentioned above, we can add new features with the function add_attrDefLabelGroup(). This takes the same arguments as list_attrDefLabelGroup() above, as well as labelGroupName (the name of the new feature), and labelGroupValues (a vector containing the relevant strings). Below we add a new feature called grave to the Phoneme level which includes all labial and velar consonants:\n\nadd_attrDefLabelGroup(ae,\n                      levelName = \"Phoneme\",\n                      attributeDefinitionName = \"Phoneme\",\n                      labelGroupName = \"grave\",\n                      labelGroupValues = c('p', 'b', 'm', 'k', 'g'))\n\nWe can now use query() to get a list of all segments with the feature grave:\n\ngrave.s1 &lt;- query(ae, \"Phoneme == grave\")\ngrave.s1 %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nThis gives the same result as specifying all grave consonants individually:\n\ngrave.s2 &lt;- query(ae, \"Phoneme == p | b | m | k | g\")\nall(grave.s1 == grave.s2)\n\n[1] TRUE\n\n\n\n\n7.3.3 Sequence queries\nSo far, we have looked only at simple queries. Anything beyond a simple query requires the query to be enclosed in square brackets [ ], while for simple queries they are optional:\n\nmn1 &lt;- query(ae, \"Phoneme == m | n\")\nmn2 &lt;- query(ae, \"[Phoneme == m | n]\")\nall(mn1 == mn2)\n\n[1] TRUE\n\n\nThe results of these two queries are identical! When querying sequences, which we do below, [ ] are no longer optional.\nThe -&gt; operator is used for finding sequences of annotations. The query [Phonetic == V -&gt; Phonetic == m], for example, finds a sequence in the Phonetic level where the segment V is followed by the segment m.\n\nquery(ae, \"[Phonetic == V -&gt; Phonetic == m]\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nBy default, sequence queries return the start time of the first segment in the sequence and the end time of the last segment in the sequence (in this case, the start time of the V segment and the end time of the m segment). This behavior can be changed with the “result-modifying” symbol #. In [#Phonetic == V -&gt; Phonetic == m], the # placed before the first portion of the query means that instead of turning V-&gt;m sequences, only instances of V which are directly followed by m are returned:\n\nquery(ae, \"[#Phonetic == V -&gt; Phonetic == m]\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nNote that here, the labels column only says V rather than V-&gt;m, and the end time matches the end of the vowel and not the end of the sequence. Conversely, in [Phonetic == V -&gt; #Phonetic == m], only instances of m which are directly preceded by V are returned:\n\nquery(ae, \"[Phonetic == V -&gt; #Phonetic == m]\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nHere, the labels column only says m, and the start time matches the beginning of the consonant.\nOnly one result-modifying symbol # is allowed per query.\n\n\n7.3.4 Embedded bracketing\nIf we want to search for sequences longer than two strings, we can use embedded bracketing. For example, if we want to find sequences of @-&gt;n-&gt;s, this needs to be phrased as a sequence of @-&gt;n and s – in other words, the sequence [Phonetic == @ -&gt; Phonetic == n] needs to be embedded in a larger query like so: [[Phonetic == @ -&gt; Phonetic == n] -&gt; Phonetic == s]:\n\nquery(ae, \"[[Phonetic == @ -&gt; Phonetic == n] -&gt; Phonetic == s]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nAs above, if we want to return only instances of s in @-&gt;n-&gt;s sequences, we can use a well-placed #:\n\nquery(ae, \"[[Phonetic == @ -&gt; Phonetic == n] -&gt; #Phonetic == s]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nAny number of queries can be combined. With the following, we look in the Text level for the word offer followed by any two annotations followed by the word resistance:\n\nquery(ae, \"[[[Text == offer -&gt; Text =~ .*] -&gt; Text =~ .*] \n      -&gt; Text == resistance]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\n\n\n7.3.5 Linked levels\nThe operator ^ is used for queries spanning two linked annotation levels. If we want to find all instances of p at the Phoneme level where the corresponding syllable is stressed (i.e., it is annotated as S at the Syllable level), this can be achieved with the query Phoneme == p ^ Syllable == S:\n\nquery(ae, \"[Phoneme == p ^ Syllable == S]\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nNote that the ^ operator does not care about the hierarchical structure of the database; the above query is not just possible because Syllable dominates Phoneme, and if we swap the order of the query, we simply get all instances of stressed syllables that dominate a p at the Phoneme level:\n\nquery(ae, \"[Syllable == S ^ Phoneme == p]\") %&gt;% select(-db_uuid)\n\n\n\n  \n\n\n\nEmbedded bracketing can be used for queries spanning more than two levels. Below we look for any phonetic annotation in a stressed syllable – i.e. [Phonetic =~ .* ^ Syllable == S] – which occurs in the word amongst or beautiful:\n\nquery(ae, \"[[Phonetic =~ .* ^ Syllable == S] ^ \n      Text == amongst |  beautiful]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\n\n\n7.3.6 Attribute queries\nThe & operator is used for annotations of a level that is an attribute of another level. For example, in our ae database, Text and Accent are attributes of the Word level. We can check this by using the function list_attributeDefinitions() to find all attributes of Word:\n\nlist_attributeDefinitions(ae, level = \"Word\")\n\n\n\n  \n\n\n\nThus, to find all accented words – i.e. words where Accent has the value S (for strong) – we look for any label in Text that also has the value S in Accent, like so:\n\nquery(ae, \"[Text =~ .* & Accent == S]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nAnd to find all unaccented (where Accent is W for weak) function words (where Word is F for function):\n\nquery(ae, \"[Text =~ .* & Accent == W & Word == F]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nWe could combine this with a sequence query if we were e.g. interested in finding all unaccented function words that immediately follow a content word (Word == C):\n\nquery(ae, \"[Word == C -&gt; #Text =~ .* & Accent == W & Word == F]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\n\n\n7.3.7 Position queries\nThe EQL has three position functions, Start(x,y), Medial(x,y), and End(x,y). The annotations are returned from y in these cases, where x and y are the two annotation levels that form part of a query. Thus, the following query finds all annotations at the Phoneme level that are initial relative to annotations at the Word level – i.e., word-initial phonemes.\n\nquery(ae, \"[Start(Word,Phoneme) == TRUE]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nWe can find all word-initial and word-medial phonemes by querying whichever phonemes are not word-final, i.e. where the End condition is FALSE:\n\nquery(ae, \"[End(Word,Phoneme) == FALSE]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nWe can combine this with other queries, such as the with the conjunction operator &, to e.g. find all instances of f and S that are word-initial, like so:\n\nquery(ae, \"[Phoneme == f | S & Start(Word,Phoneme) == TRUE]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nIf we e.g. want to find all phonemes in phrase-final syllables, we must combine a position query with a linked query, since Phoneme is dominated by Syllable. So we’d search for any Phoneme where the Syllable is at the end of an Intonational phrase, like so:\n\nquery(ae, \"[Phoneme =~ .* ^ End(Intonational,Syllable) == TRUE]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nThese can be further combined, so we could e.g. specify in the above case that we’re only interested in weak syllables (Syllable == W) like so:\n\nquery(ae, \n      \"[Phoneme =~ .* ^ Syllable == W & End(Intonational,Syllable) == TRUE]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\n\n\n7.3.8 Count queries\nCount queries can be done with the Num(x,y) function in combination with a count operator. Possible count operators are ==, != &gt;, &gt;=, &lt;, &lt;=.\nNum(x,y) will count the number of annotations at the level y relative to the level x. Unlike the position queries we just saw, which return annotations at the level y, count queries return annotations at the level x. Thus, the following command will find all bisyllabic words (annotation items at the Text level where the number of associated Syllables is exactly 2):\n\nquery(ae, \"[Num(Text,Syllable) == 2]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nThe following query will search for syllables that contain more than 4 phonemes:\n\nquery(ae, \"[Num(Syllable,Phoneme) &gt; 4]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nAgain, this can be combined with other types of queries, i.e. using a domination query to find all syllables in disyllabic words. This is because Num(Text,Syllable) will return annotations at the Text level, i.e. words.\n\nquery(ae, \"[Syllable =~ .* ^ Num(Text,Syllable) == 2]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nIt’s often helpful to build up more complex queries – that involve e.g. counts, positions, and multiple annotation levels – incrementally. This way, you can make sure along the way that you haven’t made a mistake. If, for example, we want to find the final syllable of all trisyllabic words that have a word-final /s/, we could start by finding all trisyllabic words with the count function:\n\nquery(ae, \"[Num(Text,Syllable) == 3]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nWe can add a domination query to find all of the above syllables that contain a phoneme /s/:\n\nquery(ae, \"[Num(Text,Syllable) == 3 ^ Phoneme == s]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nThis can be combined with a position query to ensure that the /s/ is word-final:\n\nquery(ae, \n      \"[Num(Text,Syllable) == 3 ^ Phoneme == s & End(Text,Phoneme) == TRUE]\") %&gt;% \n  select(-db_uuid)\n\n\n\n  \n\n\n\nAnd to make sure we get the syllable, we can add a further domination query:\n\nquery(ae, \n      \"[[#Syllable =~ .* ^ Num(Text,Syllable) == 3] \n      ^ Phoneme == s & End(Text,Phoneme) == TRUE]\") %&gt;% \n  select(-db_uuid)"
  },
  {
    "objectID": "query.html#functions-and-operators-introduced-in-this-chapter",
    "href": "query.html#functions-and-operators-introduced-in-this-chapter",
    "title": "7  The Emu query language",
    "section": "7.4 Functions and operators introduced in this chapter",
    "text": "7.4 Functions and operators introduced in this chapter\n\nrequery_hier(): Based on an existing segment list generated with a query(), performs a hierarchical requery based on linked levels.\nrequery_seq(): Based on an existing segment list generated with a query(), performs a sequential requery to search for adjacent annotations at the same level.\nlist_attrDefLabelGroups(): Lists all defined features / attribute definitions within an annotation level.\nadd_attrDefLabelGroup(): Creates a new feature / attribute definition within an annotation level.\n==, =: Exact equality operators, returns exactly matching strings\n!=: Inequality operator, returns everything but exactly matching strings\n=~: Matching operator, returns partially matching strings\n!~: Non-matching operator, returns everything but partially matching strings\n|: “or”-operator, used to combine multiple conditions\n#: Result-modifying operator, specifies which component of a query to return\n-&gt;: Sequence operator, defines a sequence of annotations to query\n^: Domination operator, defines a query based on two linked levels\nStart(x,y), Medial(x,y), End(x,y): Position operators, defines a query where the position of level x is defined relative to a dominating level y\nNum(x,y): Count operators, defines queries with specific numbers of annotation boundaries in level x relative to a dominating level y by using one of the count operators ==, !=, &lt;, &gt;, &lt;=, &gt;=."
  }
]