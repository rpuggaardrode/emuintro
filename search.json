[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An introduction to emuR",
    "section": "",
    "text": "Preface\nThis book is currently being built. It is an updated version of the materials for a course in Advanced Speech Technology taught in the MA Phonetics & Speech Processing at LMU Munich."
  },
  {
    "objectID": "setup.html#required-set-up-for-the-course-on-emur",
    "href": "setup.html#required-set-up-for-the-course-on-emur",
    "title": "Initial setup",
    "section": "Required set up for the course on emuR",
    "text": "Required set up for the course on emuR\n\nIn order to use emuR, you must first download and install the program R. The latest version is currently 4.3.x (September 2023).\nAlso download and install the program RStudio, which is a so-called integrated development environment."
  },
  {
    "objectID": "setup.html#default-browser",
    "href": "setup.html#default-browser",
    "title": "Initial setup",
    "section": "Default browser",
    "text": "Default browser\nThe EMU-webApp for visualizing sound files and editing transcriptions should in theory work with most modern browsers, but you may run into unexpected problems if you use it with other browsers than Google Chrome. For this reason, we recommend that change standard browser to ensure that the EMU-webApp is opened in Chrome.\nIf you install Chrome, you can do this from the Settings tab as shown below:"
  },
  {
    "objectID": "setup.html#start-up-rstudio",
    "href": "setup.html#start-up-rstudio",
    "title": "Initial setup",
    "section": "Start up Rstudio",
    "text": "Start up Rstudio\nStart Rstudio by clicking on the Rstudio icon – it should look like this:"
  },
  {
    "objectID": "setup.html#create-a-directory-on-your-hard-drive",
    "href": "setup.html#create-a-directory-on-your-hard-drive",
    "title": "Initial setup",
    "section": "Create a directory on your hard drive",
    "text": "Create a directory on your hard drive\nCreate a directory on your hard drive to be used on this course. You can do this from R. What exactly to call the directory will depend on your computer – this will create a directory called ipsR on the desktop of my machine (which runs Windows):\n\ndir.create('C:/users/rasmu/desktop/ipsR')\n\nIf I was a Mac user, I would have done it like this:\n\ndir.create('/Users/rasmu/Desktop/ipsR')\n\nAlternatively you can just create a directory in the usual way – e.g. by right clicking anywhere on your desktop and selecting new directory."
  },
  {
    "objectID": "setup.html#create-a-project",
    "href": "setup.html#create-a-project",
    "title": "Initial setup",
    "section": "Create a project",
    "text": "Create a project\nIn RStudio, go to File &gt; New Project.... Choose the directory you just created. See the figure below:\n\n\n\n\n\nYou only need to create the project once. When you start up Rstudio the next time, the system should automatically open up inside this project that you created. If not, or if you closed the project, you can open it again:\n\n\n\n\n\nYou can get back to your default directory (if you were using R before then) any time by selecting close project in the above menu."
  },
  {
    "objectID": "setup.html#install-packages",
    "href": "setup.html#install-packages",
    "title": "Initial setup",
    "section": "Install packages",
    "text": "Install packages\nIn the RStudio console window, run the following code to install packages needed for this course. It will take a few minutes. You only need to do this once.\n\ninstall.packages(c(\"Rcpp\", \"remotes\", \"knitr\", \n                   \"tidyverse\", \"magrittr\",\n                   \"rmarkdown\", \"emuR\", \"gridExtra\", \"emmeans\", \n                   \"broom\", \"lmerTest\", \"pbkrtest\", \"MuMIn\", \"wrassp\"))\n\n\n\n\n\n\n\nMore information: Installation of R packages\n\n\n\nShould the above result in the error message installation of package had non-zero exit status, then it means that installation has failed. For Windows, you might then additionally have to install Rtools. For iOS you might need to install/reset the XCode command-line tools. For this purpose, open a Mac terminal window and execute the following:\n\nxcode-select --install\n\nIf the installation of R packages still does not work, execute the following:\n\nxcode-select --reset\n\n\n\n\n\n\n\n\n\nThe emuhelpeR package\n\n\n\nThroughout the book there will be some tips on how to ease your work with emuR by using the functions of another package, emuhelpeR. This package is work-in-progress and the functions have not been tested thoroughly like those of emuR, and it’s not a requirement to use it. Because it’s work-in-progress, it is not (yet) available from the Comprehensive R Archive Network or CRAN, and can’t be installed with the install.packages() command. You can however install it from GitHub using the install_github() command from the devtools package.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"rpuggaardrode/emuhelpeR\")"
  },
  {
    "objectID": "setup.html#loading-libraries",
    "href": "setup.html#loading-libraries",
    "title": "Initial setup",
    "section": "Loading libraries",
    "text": "Loading libraries\nLoad in the emuR library like so:\n\nlibrary(emuR)\n\nVerify that everything works by entering the following commands to the console. The command create_emuRdemoData() downloads a demo database, and load_emuDB() loads in the database, in this case saving it as the object ae.\n\ncreate_emuRdemoData(dir = tempdir())\nae &lt;- load_emuDB(file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\"))\nserve(ae, useViewer=F)\n\nThe third command serve(ae, useViewer=F) should produce the following image in your browser:\n\nHave a look around and close the window to move on."
  },
  {
    "objectID": "setup.html#quitting-from-rstudio-and-r",
    "href": "setup.html#quitting-from-rstudio-and-r",
    "title": "Initial setup",
    "section": "Quitting from RStudio (and R)",
    "text": "Quitting from RStudio (and R)\nYou shut down RStudio by either clicking Session &gt; Quit Session in the main Rstudio toolbar or by running q() in the R console as below:\n\nq()\n\nYou will then be asked whether you want to save the workspace image. For this course, please never save the workspace i.e. (Don't save)."
  },
  {
    "objectID": "setup.html#learning-r",
    "href": "setup.html#learning-r",
    "title": "Initial setup",
    "section": "Learning R",
    "text": "Learning R\nA basic knowledge in R is a prerequisite for this course. If you are unfamiliar with R then please start by working through these two sources:\n\nA short introduction to R by Raphael Winkelmann (in English)\nA more detailed introduction to R and the tidyverse package by Johanna Cronenberg in German or English\n\nThere is a very large and helpful R community online that will make learning R easier for you. Here are a few useful links and commands in case you get stuck:\n\nStack Overflow: A blog where you can often find an answer to your questions about R. The easiest way is to google your question in English; a Stack Overflow member’s answer will be included in the first search results.\nThe book R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023): Hadley Wickham is the chief programmer of the tidyverse. His books are very readable, well-structured, and entertaining.\nCheat sheets: These are PDFs that provide an overview of functions with explanations and examples in a very compact form. You will find some cheat sheets in the main RStudio toolbar via Help &gt; Cheat Sheets. The first three are especially useful.\nVignettes: For some packages there are so-called vignettes available. These are mostly HTMLs or PDFs that have been written by the package authors. You can search for vignettes using the following input to the console (this example finds a vignette for dplyr, one of the libraries from tidyverse):\n\n\nvignette(\"dplyr\")\n\n\nYou can get information about a function by entering its name in the RStudio Help toolbar. You’ll then get information about the function’s arguments and often some examples. You can also get help via the console, as follows ( e.g. here for getwd()):\n\n\n?getwd\nhelp(\"getwd\")\n\nggplot2 is both a well-known and popular program for creating plots. There is plenty of help available for ggplot2 from the R community. Here are some useful links for creating graphics:\n\nThe chapter Visualize in Wickham, Çetinkaya-Rundel, and Grolemund (2023)\nCookbook for R\nStack Overflow\n\nR is first and foremost an environment for developed for statistical computing. If you need more information about using statistics in R, the following are recommended:\n\nWinter (2019) is a recent book with excellent explanations to all major themes in inferential statistics.\nGries (2021) is a recent update of a classic which gives a methodical introduction to both R data structures and statistical models, starting with the very basics. It is useful for decision making about which model to use for which kind of question.\nSonderegger (2023) is a very recent book which introduces regression models and discusses in depth which problems you may run into when fitting them.\nAlthough the R code in Baayen (2008) is a bit out of date, the explanations and examples of statistics foundations are very helpful.\n\n\n\n\n\n\nBaayen, R. Harald. 2008. Analyzing Linguistic Data. A Practical Introduction to Statistics Using r. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511801686.\n\n\nGries, Stefan Th. 2021. Statistics for Linguistics with r. A Practical Introduction. 3rd ed. Berlin & Boston: De Gruyter Mouton. https://doi.org/10.1515/9783110718256.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. Cambridge, MA: MIT Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. Sebastopol, CA: O’Reilly Media.\n\n\nWinter, Bodo. 2019. Statistics for Linguists. An Introduction Using r. New York & London: Routledge. https://doi.org/10.4324/9781315165547."
  },
  {
    "objectID": "intro.html#background",
    "href": "intro.html#background",
    "title": "1  A schematic overview of the Emu Speech Database Management System",
    "section": "1.1 Background",
    "text": "1.1 Background\nThe core idea behind Emu today is just the same as it was when it first launched in the mid-late 1980s at CSTR, Edinburgh University and known as APS which stood for Acoustic Phonetics in S.\nThe core idea of Emu is:\n\nA speech database is a collection of utterances consisting of signal (waveform, formant, f0 etc) and annotation (label) files.\nThere is a query language: annotations can be extracted from the database and read directly into R. It should be straightforward to e.g. find all tokens of [i] in the database.\nThese queried lists of annotations in R can be further queried to get the corresponding signals (e.g. find the formants of the [i] tokens extracted at the previous step).\n\nThere have been some landmark changes to Emu since the APS–Edinburgh days of the 1980s.\n\nIn the 1990s (at Linguistics, Macquarie University, Sydney), Emu’s query language was updated to handle hierarchically structured annotations: see especially Cassidy and Harrington (2001). (The name Emu evolved out of Extended MUlti-dimensional and because we were in Australia). The query language is still in use today and is the only one of its kind in existence.\n\n\n\n\n\n\nThe query language is still in use today and the only one of its kind in existence.\n\n\n\nThe author of the Emu query language is Steve Cassidy who can be seen here feeding the Emu ca 1996.\n\n\nThe point of hierarchical annotations is to be able to query annotations at one tier with respect to another. For example the previous query could be extended to:\n\nFind all [i] vowels in the first syllable of trisyllabic accented words, but only if they are preceded by a function word in any L% intonational phrase.\n\n\nFrom 2002–2006, the Advanced Speech Signal Processor (ASSP) toolkit developed by Michel Scheffers of the IPdS, University of Kiel was integrated into Emu (Bombien et al. 2006). ASSP then morphed into the R package wrassp ca. 2014.\nIn the last few years, the Emu engine was completely overhauled by Raphael Winkelmann with many excellent new features (see also Winkelmann, Harrington, and Jänsch 2017), e.g.:\n\n\nEmu is launched and operates entirely within the R programming environment.\nAn interactive graphical user interface for analysing and visualising data: the Emu-webApp\nExtension of the query language to include regular expressions.\nFar more rapid access to extracting annotations and their signal files from the database."
  },
  {
    "objectID": "intro.html#the-emu-sdms",
    "href": "intro.html#the-emu-sdms",
    "title": "1  A schematic overview of the Emu Speech Database Management System",
    "section": "1.2 The Emu-SDMS",
    "text": "1.2 The Emu-SDMS\n\n\n\n\n\n\nBombien, Lasse, Steve Cassidy, Jonathan Harrington, Tina John, and Sallyanne Palethorpe. 2006. “Recent Developments in the Emu Speech Database System.” Proceedings of the Australian Speech Science and Technology Conference. https://www.phonetik.uni-muenchen.de/~jmh/papers/emusst06.pdf.\n\n\nCassidy, Steve, and Jonathan Harrington. 2001. “Multi-Level Annotation in the Emu Speech Database Management System.” Speech Communication 33 (1–2): 61–77. https://doi.org/10.1016/S0167-6393(00)00069-8.\n\n\nWinkelmann, Raphael, Jonathan Harrington, and Klaus Jänsch. 2017. “EMU-SDMS. Advanced Speech Database Management and Analysis in r.” Computer Speech & Language 45: 392–410. https://doi.org/10.1016/j.csl.2017.01.002."
  },
  {
    "objectID": "first_steps.html#preliminaries-and-loading-libraries",
    "href": "first_steps.html#preliminaries-and-loading-libraries",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.1 Preliminaries and loading libraries",
    "text": "2.1 Preliminaries and loading libraries\nFollow the setup instructions given in the setup chapter, i.e. download R and RStudio, create a directory on your computer where you will store files for this course, make a note of the directory path, create an R project that accesses this directory, and install all indicated packages. \nFor this and subsequent tutorials, access the tidyverse,magrittr, emuR, and wrassp libraries:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(emuR)\nlibrary(wrassp)"
  },
  {
    "objectID": "first_steps.html#accessing-an-existing-emu-speech-database",
    "href": "first_steps.html#accessing-an-existing-emu-speech-database",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.2 Accessing an existing Emu speech database",
    "text": "2.2 Accessing an existing Emu speech database\nFor a first look at the properties of Emu databases, we will use the function create_emuRdemodata() to download and access a demonstration database.\n\ncreate_emuRdemoData(dir = tempdir())\n\nThe option dir = tempdir() tells R to store the demo database in a temporary directory, the path of which is determined internally by the tempdir() function.\nWe can now use the command list.dirs() to have a look inside the temporary directory and see which directories are in there. (recursive=FALSE tells R not to return the names of all subdirectories, and full.names=FALSE tells R not to return the full paths to directories).\n\nlist.dirs(tempdir(), recursive=FALSE, full.names=FALSE)\n\n[1] \"emuR_demoData\"\n\n\nOur demo data is stored in the first subdirectory, emuR_demoData. Let’s have a look at what is stored in there. (The second directory with the long obscure name is always created when a new R session is started).\n\n#store path name as an object\npath.emuDB &lt;- paste0(tempdir(), \"/emuR_demoData\")\nlist.dirs(path.emuDB, recursive=FALSE, full.names=FALSE)\n\n[1] \"ae_emuDB\"            \"BPF_collection\"      \"legacy_ae\"          \n[4] \"TextGrid_collection\" \"txt_collection\"     \n\n\nThere are some folders containing collections of sound files and annotation files. (In Chapter 4 we will see how to convert an existing collection of sound files and TextGrids to an Emu database). There is also a folder called ae_emuDB, which contains our actual demo database. The database is called ae, and the string _emuDB is always appended to Emu databases. The full path to this Emu database can be accessed with the file.path() function, like so:\n\nfile.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\n\n[1] \"C:\\\\Users\\\\rasmu\\\\AppData\\\\Local\\\\Temp\\\\RtmpkhTwaR/emuR_demoData/ae_emuDB\"\n\n\nIn order to see the files that are physically stored in the ae_emuDB, first save the path name and then use the function list.files():\n\npath.ae &lt;- file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\nlist.files(path.ae)\n\n[1] \"0000_ses\"         \"ae_DBconfig.json\"\n\n\nThe file ae_DBconfig.json is a template that stores information about the defining properties of the ae database; see Section 2.3. The utterances are in this case all stored in the directory 0000_ses. To look inside this directory:\n\nlist.files(file.path(path.ae, \"0000_ses\"))\n\n[1] \"msajc003_bndl\" \"msajc010_bndl\" \"msajc012_bndl\" \"msajc015_bndl\"\n[5] \"msajc022_bndl\" \"msajc023_bndl\" \"msajc057_bndl\"\n\n\nThis shows that there 0000_ses contains 7 directories of so-called bundles bndl. Emu organises things such that all the files that belong to the same utterance are always in the same bundle. The utterance identifier precedes _. Thus in this case, it is clear from the output above that there are 7 utterances whose identifiers are msajc003, msajc010, msajc012, msajc015, msajc023, msajc057. We can again use list.files() in combination with file.path() to see what files there are for any utterance, in this case for msajc003:\n\nlist.files(file.path(path.ae, \"0000_ses\", \"msajc003_bndl\"))\n\n[1] \"msajc003.dft\"        \"msajc003.fms\"        \"msajc003.wav\"       \n[4] \"msajc003_annot.json\"\n\n\nThis shows that msajc003 has the following files:\n\n_annot.json: this stores information about the annotations.\n.dft: this contains a derived signal file of DFT (spectral) data obtained from the speech waveform; spectral processing is discussed in more detail in Chapter 12.\n.fms: this contains formant data also obtained from the speech waveform.\n.wav: this contains the waveform itself.\n\n.dft and .fms are just examples of the sorts of signal files that can be stored in an Emu database – this could also have been signals like pitch tracks or intensity, but for the advanced user, essentially any signal can be stored in an Emu database. We will see more examples in Chapter 8.\nIn order to access the ae emuDB in R, the above path needs to be stored and passed to the function load_emuDB() that reads the database into R. The output of load_emuDB() should always be assigned to an object in R, or you will not be able to access it later. Here we store it as ae.\n\n# store the above path name\npath2ae &lt;- file.path(tempdir(), \"emuR_demoData\", \"ae_emuDB\")\nae &lt;- load_emuDB(path2ae, verbose=FALSE)\n\n\n\n\n\n\n\nWhat does verbose=FALSE mean?\n\n\n\nThroughout this book you will often see us use the option verbose=FALSE when calling functions from emuR. By default emuR is pretty talkative (or verbose), meaning it will print a lot of information in the R console about the status of whatever task it is performing. This is usually quite nice, and you may want to avoid the verbose=FALSE option if you are coding along in your own R session – we mostly keep it quiet here because those messages would otherwise clutter up the book."
  },
  {
    "objectID": "first_steps.html#sec-defprops",
    "href": "first_steps.html#sec-defprops",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.3 Some defining properties of an Emu database",
    "text": "2.3 Some defining properties of an Emu database\nThe well-known function summary() can also be used with an Emu database to summarise its salient attributes:\n\nsummary(ae)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ae \nUUID:    0fc618dc-8980-414d-8c7a-144a649ce199 \nDirectory:   C:\\Users\\rasmu\\AppData\\Local\\Temp\\RtmpkhTwaR\\emuR_demoData\\ae_emuDB \nSession count: 1 \nBundle count: 7 \nAnnotation item count:  736 \nLabel count:  844 \nLink count:  785 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\n name columnName fileExtension\n dft  dft        dft          \n fm   fm         fms          \n\n\n── Level definitions ──\n\n\n name         type    nrOfAttrDefs attrDefNames       \n Utterance    ITEM    1            Utterance;         \n Intonational ITEM    1            Intonational;      \n Intermediate ITEM    1            Intermediate;      \n Word         ITEM    3            Word; Accent; Text;\n Syllable     ITEM    1            Syllable;          \n Phoneme      ITEM    1            Phoneme;           \n Phonetic     SEGMENT 1            Phonetic;          \n Tone         EVENT   1            Tone;              \n Foot         ITEM    1            Foot;              \n\n\n── Link definitions ──\n\n\n type         superlevelName sublevelName\n ONE_TO_MANY  Utterance      Intonational\n ONE_TO_MANY  Intonational   Intermediate\n ONE_TO_MANY  Intermediate   Word        \n ONE_TO_MANY  Word           Syllable    \n ONE_TO_MANY  Syllable       Phoneme     \n MANY_TO_MANY Phoneme        Phonetic    \n ONE_TO_MANY  Syllable       Tone        \n ONE_TO_MANY  Intonational   Foot        \n ONE_TO_MANY  Foot           Syllable    \n\n\nThe directory shows the path where the directory is located.\nBundle count shows how many utterances there are in the database. We can query their names with the function list_bundles.\n\nlist_bundles(ae)\n\n\n\n  \n\n\n\nThe SSFF track definitions show which signals are currently available in the database apart from the waveforms. For this database there are signals of type dft (discrete Fourier transform, see Chapter 12) and of type fm (formant). They have extensions .dft and .fms. This information is also given by:\n\nlist_ssffTrackDefinitions(ae)\n\n\n\n  \n\n\n\nThe Level definitions show the available annotation levels of the database (equivalent to tiers in Praat). This information is also given by:\n\nlist_levelDefinitions(ae)\n\n\n\n  \n\n\n\nAs the above shows, annotation levels can be of three types: ITEM, SEGMENT, and EVENT. The annotations of ITEM levels inherit their times from the (typically) SEGMENT levels that they dominate. In a SEGMENT levels, each annotation has a start and end time, equivalent to segment tiers in Praat. In an EVENT level, each annotation is defined by a single point in time, equivalent to point tiers in Praat. A level can also be associated with one or more ATTRIBUTE levels. This information is also given by the function list_attributeDefinitions() with the database name as the first argument, and the level to be queried for attributes as the second. For example:\n\nlist_attributeDefinitions(ae, \"Word\")\n\n\n\n  \n\n\n\nThis shows that the levels Accent and Text are attributes of Word. The annotations of an ATTRIBUTE level always have identical times to those of the main level with which they are associated (thus the annotations of the Accent level have identical start and end times to those of the Word level). An ATTRIBUTE level is often used to provide additional information about annotations. In the ae database, the annotations in the Word level consist entirely of either C (content word) or F (function word). The annotations at the Text level are used to provide the orthography for each content or function word annotation; and the annotations of the Accent level are used to mark whether or not a word is prosodically accented.\nThe information in Link definitions of summary(ae) shows how the levels are associated with each other. This information is also provided by the function list_linkDefinitions():\n\nlist_linkDefinitions(ae)\n\n\n\n  \n\n\n\nIn the Emu system, annotation levels can be (but need not be) hierarchically organised with respect to each other. One reason for the hierarchical organisation of levels is to allow annotations to inherit times if these are predictable.\nIn the left part of Figure 2.1 for example, the start and end times of the annotation seen in the Text level are completely predictable from the annotations at the Phonetic level that is dominated by Text. Text is an item level, Phonetic is a segment level as indicated by (S). Text dominates Phonetic as shown by the vertical downward arrow. Compatibly, the Emu system provides a way for the start and end times of seen to be inherited from the annotations at the hierarchically lower SEGMENT level Phonetic. Because Text is an ITEM level that dominates Phonetic which is a SEGMENT level, annotations at the Text level inherit their times from Phonetic. Consequently, the duration of seen is \\(t_4 - t_1\\). Text and Phonetic stand in a ONE_TO_MANY association (as signified by the downward arrow) because an annotation at the Text level can be associated with one or more annotations at the Phonetic level, but not vice versa.\n\n\n\nFigure 2.1: Example relations between annotation levels.\n\n\nOn the right, Phoneme and Phonetic stand in a MANY_TO_MANY relationship (as indicated by the double arrow) because an annotation at the Phoneme tier can map to more than one annotation at the Phonetic level and vice versa. In this hypothetical example of an annotation of the second syllable of a word like region, the single affricate annotation /dZ/ at the Phoneme level maps to a sequence of [d] and [Z] annotations at the Phonetic level, while the single annotation of the syllabic [n] at the Phonetic level maps to a sequence of annotations /@n/ at the Phoneme tier. Note that /@/ and /n/ inherit the same start and end times and therefore have the same duration of \\(t_4 - t_3\\) i.e. they overlap with each other in time completely.\nAnother reason for the hierarchical organisation of annotation levels is that it allows users to query the database in order to obtain annotations at one tier with respect to another (e.g., all orthographic annotations of the vowels in the database; all H* pitch accents in an intermediate phrase, etc.). Without this linkage, these types of queries would not be possible.\nEmu allows quite a flexible configuration of annotation levels. The type of configuration can be defined by the user and will depend on the types of information that the user wants to be able to extract from the database. The configuration of annotation levels for the currently loaded ae database is shown in Figure 2.2.\n\n\n\nFigure 2.2: The links between the annotation levels of the ae database. ITEM levels are unmarked, SEGMENT levels are marked with (S) and EVENT levels with (E). ATTRIBUTE levels have no arrow between them (thus Text and Accent are attribute levels of Word). A downward arrow signifies domination in a ONE_TO_MANY relationship; a double arrow signifies domination in a MANY_TO_MANY relationship.\n\n\nInherited times percolate up through the tree from levels with associated time information, i.e. from SEGMENT and EVENT levels upwards through ITEM levels. Thus, Phoneme is an item level which inherits its times from the SEGMENT level Phonetic. Word inherits its times from Syllable which inherits its times from Phoneme (and therefore from Phonetic) and so on all the way up to the top level Utterance. Sometimes, levels can inherit more than one set of times. In Figure 2.2, Syllable inherits times both from Phonetic (S) and from Tone (E). For the same reason, all the levels that dominate Syllable (including Foot) inherit these two sets of times.\nAny two annotation levels on the same path can be queried with respect to each other, where a path is defined as levels connected by arrows. There are in fact four paths in the configuration:\n\nUtterance → Intonational → Intermediate → Word → Syllable → Phoneme ↔︎ Phonetic (S)\nUtterance → Intonational → Root → Syllable → Phoneme ↔︎ Phonetic (S)\nUtterance → Intonational → Syllable → Tone (E)\nUtterance → Intonational → Intermediate → Word → Syllable → Tone (E)\n\nFrom (1–4), it becomes clear that e.g. annotations of the Syllable tier can be queried with respect to Tone (e.g. which syllables contain an H* tone?) and vice versa (e.g. are there any H* tones in weak syllables?); or annotations at the Intermediate tier can be queried with respect to Word (how many words are there in an L- intermediate phrase?) and vice versa (which words are in an L- intermediate phrase?). But e.g. Phoneme and Tone can’t be queried with respect to each other, and nor can Word and Foot, because they aren’t on the same path."
  },
  {
    "objectID": "first_steps.html#viewing-and-annotating-an-emu-database",
    "href": "first_steps.html#viewing-and-annotating-an-emu-database",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.4 Viewing and annotating an Emu database",
    "text": "2.4 Viewing and annotating an Emu database\nAn Emu database can be viewed and annotated in at least two ways using the serve() function. Simply passing the database object to serve() will open the Emu database within the R graphics window:\n\nserve(ae)\n\nYou’ll probably usually prefer to watch the database in a larger window; if you include the argument useViewer=FALSE, it’ll open in your default browser instead (which should preferably be Chrome). Alternatively you can click the Show in new window bottom in the RStudio viewer.\n\nserve(ae, useViewer=F)\n\n\n\n\nFigure 2.3: The ae database\n\n\n\n\nIt is not the purpose of this introduction to give explicit instruction on how to annotate which is covered amply in the manual for the Emu Speech database management system, especially section 9.\nHowever, some basic properties can be noted. These include:\n\nThere are, as mentioned before, 7 utterances.\nThe database displays two types of signals: the waveform, and the spectrogram below it.\n\nThis information about the signals being displayed is also given by the function get_signalCanvasesOrder(). This function takes the obligatory argument perspectiveName, which should be \"default\" unless you have created custom signal canvases (discussed further below).\n\nget_signalCanvasesOrder(ae, perspectiveName = \"default\")\n\n[1] \"OSCI\" \"SPEC\"\n\n\nOSCI is the waveform and SPEC the spectrogram. These can be changed with the order argument in the set_signalCanvasesOrder() For example, if we want to display only the spectrogram, we can do it like so:\n\nset_signalCanvasesOrder(ae, perspectiveName = \"default\", order = \"SPEC\")\n\nIf you serve() the database again, you should see only the spectrogram:\n\nserve(ae, useViewer = F)\n\nTo restore the original order, call set_signalCanvasesOrder() again:\n\nset_signalCanvasesOrder(ae, perspectiveName = \"default\", \n                        order = c(\"OSCI\", \"SPEC\"))\n\n\n\n\n\n\n\nAdding signal canvas with emuhelpeR\n\n\n\nOne of the most common use cases of set_signalCanvasesOrder() is to add a signal (such as the formant track fm) to the default perspective for a quick look at it. This is a little cumbersome with set_signalCanvasesOrder(), but a helper function add_signal_canvas() can be found in the emuhelpeR package. This function takes just two arguments: the database object, and the name(s) of signal tracks to add to the default perspective.\n\nlibrary(emuhelpeR)\nadd_signal_canvas(ae, add=\"fm\")\n\nTo reset the default perspective so that it shows just a waveform and spectrogram, emuhelpeR has the helper function reset_signal_canvas:\n\nreset_signal_canvas(ae)\n\n\n\nEmu will only ever display time levels with signals (in this case there are two: Phonetic and Tone that are SEGMENT and EVENT tiers respectively). The time levels to be displayed and their order can be shown and changed with the functions get_levelCanvasesOrder() and set_levelCanvasesOrder().\n\nget_levelCanvasesOrder(ae, perspectiveName = \"default\")\n\n[1] \"Phonetic\" \"Tone\"    \n\n\nWe can change the order of the Phonetic and Tone levels like so:\n\nset_levelCanvasesOrder(ae, perspectiveName = \"default\", \n                       order = c(\"Tone\", \"Phonetic\"))\n\nOr display only the tone level, like so:\n\nset_levelCanvasesOrder(ae, perspectiveName = \"default\", order = \"Tone\")\n\nThe ITEM annotation levels can all be seen in the hierarchy view. The four paths identified earlier are visible when clicking on the triangle on the far right (Figure 2.3). Clicking the triangle can also be used to change to another path. The attribute levels can be seen by clicking on one of the level names displayed at the top – e.g. click on Word to show the attribute levels Text and Accent with which the Word level is associated.\n\n\n\n\n\n\nemuR and pipes\n\n\n\nIf you are a regular user of the tidyverse libraries, you are probably familiar with the pipe operator %&gt;%. You may have noticed that the first argument of emuR functions is almost always the emuDBhandle. This means that you can pipe the database object into these functions, for example\n\nae %&gt;% list_ssffTrackDefinitions\n\n\n\n  \n\n\nae %&gt;% list_levelDefinitions\n\n\n\n  \n\n\nae %&gt;% get_levelCanvasesOrder(\"default\")\n\n[1] \"Tone\"\n\n\nSimilarly, you can view a database by calling ae %&gt;% serve."
  },
  {
    "objectID": "first_steps.html#functions-introduced-in-this-chapter",
    "href": "first_steps.html#functions-introduced-in-this-chapter",
    "title": "2  First steps in using the Emu Speech Database Management System",
    "section": "2.5 Functions introduced in this chapter",
    "text": "2.5 Functions introduced in this chapter\n\ncreate_emuRdemoData(): downloads an online demo database\nlist.dirs() and list.files(): lists the directories or files within a directory on your computer\nfile.path(): returns the location of a directory on your computer\nload_emuDB(): loads an emuDB into the R environment\nsummary() lists the salient contents of an emuDB\nlist_bundles(): lists the so-called bundles of an emuDB\nlist_levelDefinitions(): lists the annotation levels of tiers of an emuDB\nlist_attributeDefinitions(): lists the attribute tiers of an annotation tier\nlist_linkDefinitions(): lists the links between the tiers of an emuDB\nlist_ssffTrackDefinitions(): lists the available signal files of an emuDB\nserve(): view and annotate an emuDB\nget_signalCanvasesOrder(): shows what signals are being displayed when serve() is launched.\nset_signalCanvasesOrder(): changes the signals to be displayed.\nadd_signal_canvas(): adds one or more signals to the default display perspective (requires the emuhelpeR package)\nreset_signal_canvas(): resets the default display perspective so it shows the waveform and spectrogram (requires the emuhelpeR package)\nget_levelCanvasesOrder(): shows what annotation levels are being displayed when serve() is launched.\nset_levelCanvasesOrder(): changes the annotation levels to be displayed."
  },
  {
    "objectID": "wav2emu.html#preliminaries",
    "href": "wav2emu.html#preliminaries",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.1 Preliminaries",
    "text": "3.1 Preliminaries\nIf you want to code along with this chapter, you should download and unzip the file testsample (accessible here). Store the testsample folder in the ipsR directory you made when you did the initial setup for the tutorial. Also create as directory called emu_databases on your machine in the ipsR directory. The directory should now look like this:\n\n\n\n\n\nNow start up R and load the following libraries:\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\n\nIn R, store the path to the directory testsample under the name sourceDir in the following way:\n\nsourceDir &lt;- \"./testsample\"\n\nAnd also store in R the path to emu_databases as targetDir:\n\ntargetDir &lt;- \"./emu_databases\"\n\nNote that if you source this tutorial directly from GitHub, these documents and file structures are already available in the tutorial R project."
  },
  {
    "objectID": "wav2emu.html#creating-an-emu-database-from-scratch",
    "href": "wav2emu.html#creating-an-emu-database-from-scratch",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.2 Creating an Emu database from scratch",
    "text": "3.2 Creating an Emu database from scratch\nIn this chapter, we will see how to build an Emu database from scratch.\nOne of the test samples you downloaded earlier is called german. Store the path to this directory as below:\n\npath.german &lt;- file.path(sourceDir, \"german\")\n\nThis database contains the following sound files (we use the pattern=\"*wav\" argument to get only files where the name ends in wav).\n\nlist.files(path.german, pattern = \"*wav\")\n\n[1] \"K01BE001.wav\" \"K01BE002.wav\"\n\n\nWe create an empty database using the create_emuDB() command:\n\ncreate_emuDB(name = \"german\", \n             targetDir = targetDir)\n\nThis can now be loaded in using the load_emuDB() command we saw in the previous chapter.\n\ngerman_DB &lt;- load_emuDB(file.path(targetDir, \"german_emuDB\"))\n\nWhat’s in the database? Nothing!\n\nsummary(german_DB)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    german \nUUID:    7fd56a9f-afc9-40fd-95ff-88af18273590 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\german_emuDB \nSession count: 0 \nBundle count: 0 \nAnnotation item count:  0 \nLabel count:  0 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nWe can import the WAV files in our German test sample into the database using the import_mediaFiles() command:\n\nimport_mediaFiles(german_DB, dir = path.german, verbose = FALSE)\n\nWhat’s in the database now? Not nothing!\n\nsummary(german_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    german \nUUID:    7fd56a9f-afc9-40fd-95ff-88af18273590 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\german_emuDB \nSession count: 1 \nBundle count: 2 \nAnnotation item count:  0 \nLabel count:  0 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nThe following bundles are available:\n\nlist_bundles(german_DB)\n\n\n\n  \n\n\n\nNote that all bundles are associated with a session. If the sound files all come from a single directory, they are assigned to a session called 0000. If our testsample/german directory had had multiple subdirectories, these would be assigned to different sessions with the same names as the subdirectories.\nYou can now serve() the database and have a look at the data.\n\nserve(german_DB, useViewer = F)\n\nWe’re almost always interested in adding annotations to our databases. You can add an annotation level using the add_levelDefinition() function. Annotation levels need a name (we call this one Phon) and a type. The type is SEGMENT because we want to add annotation units with start and end times; the different types of annotation levels (SEGMENT, ITEM, and EVENT) were presented in the previous chapter.\n\nadd_levelDefinition(german_DB, \n                    name = \"Phon\", \n                    type = \"SEGMENT\",\n                    verbose=FALSE)\n\nWill this new annotation level be shown when we serve() the database again? We can check this with get_levelCanvasesOrder(), as seen in the previous chapter\n\nget_levelCanvasesOrder(german_DB, \n                       perspectiveName = \"default\")\n\nNULL\n\n\nNope! New annotation levels are not shown by default, but need to be explicitly added to a perspective with the function set_levelCanvasesOrder().\n\nset_levelCanvasesOrder(german_DB, \n                       perspectiveName = \"default\", \n                       order = \"Phon\")\n\nIf we now serve() the database, the Phon level should be visible.\n\nserve(german_DB, useViewer = F)\n\nIt’s still empty though. Try to annotate some segments as in Figure 3.1, e.g. /O, a/ for the vowels of Sonne and lacht in K01BE002 and save the annotations.\n\n\n\nFigure 3.1: An utterance with two annotations from german_emuDB.\n\n\n\n\n\n\n\n\nSome information about annotating in Emu\n\n\n\nThe procedure for entering annotations into Emu may not be immediately intuitive especially if you are used to doing this in Praat. Once you get the hang of it, though, it is very fast!\nCreating an interval can be done as in Praat. Place your cursor at the desired start point of an interval, then left click your mouse and hold it while dragging your cursor to the desired end point of an interval, and then let go of the clicker. (If this sounds complicated – it isn’t! Don’t overthink it, just think of it as the usual way to mark some interval with your mouse.) Now hit the enter key on your keyboard. This should draw two vertical lines signaling the boundaries of your segment.\nIf you don’t want to create an interval but simply one boundary, simply place your cursor somewhere in the spectrogram or waveform, left click your mouse, and hit the enter key. This should draw a single vertical line.\nDeleting a boundary can be done by simply placing your cursor near the boundary you want to delete. You should see it turn from right to blue – this means that it is selected, you don’t need to click it. Now hit the backspace key on your keyboard. This should delete the boundary, and if relevant, it will combine the text in two adjacent intervals. If you want to delete an interval including the annotations in it, left click somewhere within the interval and press Shift + Backspace. This will expand the surrounding intervals.\nTo enter a label, left click inside an interval, and hit the enter key. Alternatively you can use your arrow keys to move left and right between intervals, and hit the enter keys when you have found the one you want to edit. If you have just created a boundary, it is automatically selected, and you can hit enter right away to start entering a label. The interval will now turn bright yellow, and you can type in text. When you are done typing, hit the enter key again.\nTo move a boundary, simply place your cursor near a boundary as explained above, hold down the Shift key, and move your cursor left to right. Let go of the Shift key when the boundary is in the right position.\nTo save an annotation, either hit Shift + S, or left click on the Save icon next to the bundle name in the left side your Emu window; it will have turned red.\n\n\nAssuming you have managed to annotate the segments as in Figure 3.1 above, the segments should now be accessible in R with the function query(). (As mentioned in Chapter 1, the Emu query language is one-of-a-kind and very flexible! It will be discussed in much more detail in Chapter 7.)\n\nquery(german_DB, \"Phon =~ .*\")"
  },
  {
    "objectID": "wav2emu.html#adding-word-annotations",
    "href": "wav2emu.html#adding-word-annotations",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.3 Adding word annotations",
    "text": "3.3 Adding word annotations\nThe next task is to add orthographic labels as ITEM annotations. This should be done if either (a) the start and end times are of no concern and/or (b) a word’s start and end time are inherited from segments. There are three steps:\nAdd a new level to the database using the command add_levelDefinition(). Here we add one with the name ORT:\n\nadd_levelDefinition(german_DB, \n                    name = \"ORT\", \n                    type = \"ITEM\",\n                    verbose=FALSE)\n\nDefine how it is linked with a time-based level, in this case with Phon. We do this with the command add_linkDefinition. The different types of link were discussed in Chapter 2.\n\nadd_linkDefinition(german_DB, \n                   type = \"ONE_TO_MANY\", \n                   superlevelName = \"ORT\", \n                   sublevelName = \"Phon\")\n\nFinally, we can check that the hierarchical links are as expected using the function list_linkDefinitions().\n\nlist_linkDefinitions(german_DB)\n\n\n\n  \n\n\n\n\n3.3.1 Adding ITEM annotations via the EMU-webApp\nOpen the hierarchy window for utterance K01BE001, and click on the blue and white + sign next to ORT. Each time you do so, a node appears. You can enter annotation text for any node by positioning the mouse over it and then left click to bring up a green rectangle (see figure below) into which you can type text and then hit enter. To delete a node, move the mouse over it and hit the y key. Further details can be seen in the figure below.\n\nIf you have done something as in the above figure, you should be able to access the annotations in R. Notice the NA under start and end times. This is because they are timeless, i.e. unlinked to any annotations of a time (SEGMENT or EVENT) tier.\n\nquery(german_DB, \"ORT=~.*\")\n\n\n\n  \n\n\n\n\n\n3.3.2 Adding ITEM annotations via the emuR\nYou can also add timeless annotations (in this case to the ORT tier) with the function create_itemsInLevel(). This requires passing a data frame with certain specific requirements to the itemsToCreate argument. This data frame should have the columns labels, session, bundle, level, start_item_seq_idx, and attribute, with one row for each annotation to be added. In this example, the words die Sonne lacht will be added to the second utterance.\nLet’s make a data frame with this information step-by-step. First, we store the sentence as a vector w with a string corresponding to each word:\n\nw &lt;- c(\"die\", \"Sonne\", \"lacht\")\n\nWhat session do the labels belong to? In this case there is only one session, 0000. Make a vector with the same length as w which repeats the session name.\n\nsess &lt;- rep(\"0000\", length(w))\n\nWhat bundle do the labels belong to?\n\nbundle &lt;- rep(\"K01BE002\", length(w))\n\nWhat’s the name of the annotation level? This will be ORT as created above, and in this case the attribute name is identical to the level name (see Chapter 1 for more details).\n\nlev &lt;- rep(\"ORT\", length(w))\n\nWhat order do the annotations occur in? This is 1, 2, 3 for die Sonne lacht.\n\ninds &lt;- 1:length(w)\n\nPut all the above information into a data frame as follows:\n\nnewItems_ORT &lt;- data.frame(session = sess, \n                          bundle = bundle, \n                          level = lev, \n                          start_item_seq_idx = inds,\n                          attribute = lev, \n                          labels = w,\n                          stringsAsFactors = F)\nnewItems_ORT\n\n\n\n  \n\n\n\nAdd these word annotations to the database:\n\ncreate_itemsInLevel(german_DB, newItems_ORT, \n                    verbose=FALSE)\n\n\n\n\n\n\n\nUsing create_itemsInLevel() with SEGMENT or EVENT levels\n\n\n\nYou can also add annotation items for annotation levels with time information, i.e. SEGMENT or EVENT levels. This requires a data frame with the same columns as newItems_ORT which we created above, but instead of having numeric indexes in the column start_item_seq_idx, this column should contain the start time in ms of the selected boundary. (In the case of an EVENT level, this should be called start instead of start_item_seq_idx).\nIf you’re creating annotation items programmatically in a SEGMENT tier in this way, you’ll likely also want to add an end time. Somewhat counterintuitively, you do this by adding another row to the data frame with an empty label. If we wanted to use create_itemsInLevel() to create the vowel labels in K01BE002 that we made manually below, we would do it like this:\n\nw &lt;- c(\"O\", \"\", \"a\", \"\")\nsess &lt;- rep(\"0000\", length(w))\nbundle &lt;- rep(\"K01BE002\", length(w))\nlev &lt;- rep(\"Phon\", length(w))\ntimes &lt;- c(1048.5, 1111.7, 1275, 1391.1)\n\nnewItems_Phon = data.frame(session = sess, \n                          bundle = bundle, \n                          level = lev, \n                          start_item_seq_idx = times,\n                          attribute = lev, \n                          labels = w,\n                          stringsAsFactors = F)\nnewItems_Phon\n\n\n\n  \n\n\n\nThe newItems_Phon can now be passed onto the create_itemsInLevel() function as we saw above. This of course requires us to know our boundary times already, but one can easily imagine a situation where signal processing tools are used to find landmarks in sound files, and these landmarks are then used to create boundaries.\n\n\n\nIf we use query() as above, we should now see words in both bundles:\n\nquery(german_DB, \"ORT=~.*\")\n\n\n\n  \n\n\n\nHave a look at the database again:\n\nserve(german_DB, useViewer = F)\n\nLook at the hierarchy for the second utterance. The word annotations in the ORT tier should now be visible. These can be linked manually in the Emu-WebApp so that the word labels are accessible in emuR. Please see section 9.2.2 of the Emu SDMS manual for details on how to annotate hierarchically.\n\n\n\n\n\n\nSome information about annotating hierarchically\n\n\n\nAdding hierarchical links and annotations is not difficult. The present task is to add links from O and from a at the Phon tier to Sonne and to lacht respectively at the ORT tier. To do this for the first of these, hover the mouse over Sonne (the node will turn blue), hold down the shift key, and sweep the mouse to O (whose node will also turn blue), release the shift key, and the link is made. If you want to delete the link, hover the mouse over it (the link will then turn bright yellow) and hit backspace.\nYou can add new nodes at the ORT tier as follows: Move the mouse over lacht, type n and hit enter. This will create a new node before lacht (between Sonne and lacht). If you enter m instead of n in the above operation, a new node will be created after lacht. To edit or play a node at the ORT tier, left click on the node. You can enter or modify text in the green panel. To denote a node at the ORT tier, hover over it and enter y. See figure below.\n\n\n\nIf you have annotated as in the above figure and saved it, the words and their times will be accessible (note that the times at the ORT level and the same as the times at the Phon level, because each word only dominates one segment (and inherits its times from those). Note also that die has no times, because it hasn’t been linked to any annotations at the Phon level. We can check this with query:\n\nquery(german_DB, \"ORT =~ .*\", bundlePattern = \"K01BE002\")"
  },
  {
    "objectID": "wav2emu.html#functions-introduced-in-this-chapter",
    "href": "wav2emu.html#functions-introduced-in-this-chapter",
    "title": "3  Creating an Emu database from .wav files",
    "section": "3.4 Functions introduced in this chapter",
    "text": "3.4 Functions introduced in this chapter\n\ncreate_emuDB(): makes an emuDB from scratch\nimport_mediaFiles(): adds sound files to an emuDB\ncreate_itemsInLevel(): allows for annotating an emuDB programmatically from R.\nquery(): queries the annotation levels in an emuDB. This will be discussed in much more detail in Chapter 7."
  },
  {
    "objectID": "praat2emu.html#objective-and-preliminaries",
    "href": "praat2emu.html#objective-and-preliminaries",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.1 Objective and preliminaries",
    "text": "4.1 Objective and preliminaries\nThe aim of this chapter is to show how to get from a Praat .TextGrid format to an Emu database format. We will show how to convert pairs of .wav and .TextGrid files to an Emu database, how to generate pitch tracks over the sound files, how to add annotation levels in the style of Praat’s point tiers, and finally how to convert the flat annotation structure of Praat TextGrids into the layered annotation structure of an Emu database (see Chapter 1).\nas exemplified in Figure 4.1:\n\n\n\nFigure 4.1: An utterance fragment in Praat and in Emu.\n\n\nThe assumption is that you already have an R project called ipsR and that it contains the directories emu_databases and testsample. If this is not the case, please go back and follow the preliminaries chapter.\n\nStart up R in the project you are using for this course.\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\n\nIn R, store the path to the directory testsample as sourceDir in the following way:\n\nsourceDir &lt;- \"./testsample\"\n\nAnd also store in R the path to emu_databases as targetDir:\n\ntargetDir &lt;- \"./emu_databases\""
  },
  {
    "objectID": "praat2emu.html#converting-praat-textgrids",
    "href": "praat2emu.html#converting-praat-textgrids",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.2 Converting Praat TextGrids",
    "text": "4.2 Converting Praat TextGrids\nThe directory testsample/praat on your computer contains a Praat-style database with .wav files and .TextGrid files. Define the path to this database in R and check that you can see these files with the list.files() function:\n\npath.praat &lt;- file.path(sourceDir, \"praat\")\nlist.files(path.praat)\n\n [1] \"wetter1.TextGrid\"  \"wetter1.wav\"       \"wetter10.TextGrid\"\n [4] \"wetter10.wav\"      \"wetter11.TextGrid\" \"wetter11.wav\"     \n [7] \"wetter12.TextGrid\" \"wetter12.wav\"      \"wetter13.TextGrid\"\n[10] \"wetter13.wav\"      \"wetter14.TextGrid\" \"wetter14.wav\"     \n[13] \"wetter15.TextGrid\" \"wetter15.wav\"      \"wetter16.TextGrid\"\n[16] \"wetter16.wav\"      \"wetter17.TextGrid\" \"wetter17.wav\"     \n[19] \"wetter2.TextGrid\"  \"wetter2.wav\"       \"wetter3.TextGrid\" \n[22] \"wetter3.wav\"       \"wetter4.TextGrid\"  \"wetter4.wav\"      \n[25] \"wetter6.TextGrid\"  \"wetter6.wav\"       \"wetter7.TextGrid\" \n[28] \"wetter7.wav\"      \n\n\nThe emuR function for converting the collection of .wav–.TextGrid pairs to an Emu database called and then storing the latter in targetDir (defined above) is convert_TextGridCollection(). It works like this:\n\nconvert_TextGridCollection(path.praat, \n                           dbName = \"praat\",\n                           targetDir = targetDir,\n                           verbose=FALSE)\n\nThe converted Praat database called praat_emuDB can now be loaded with load_emuDB():\n\npraat_DB &lt;- load_emuDB(file.path(targetDir, \"praat_emuDB\"),\n                       verbose=FALSE)\n\nAnd its properties can be examined with the summary() command as we’ve seen in previous chapters.\n\nsummary(praat_DB)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    praat \nUUID:    7a89466f-68ec-4310-9a93-7a32fb5fec3a \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\praat_emuDB \nSession count: 1 \nBundle count: 14 \nAnnotation item count:  214 \nLabel count:  214 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name type    nrOfAttrDefs attrDefNames\n ORT  SEGMENT 1            ORT;        \n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nAnd it can of course be viewed, using the serve() command.\n\nserve(praat_DB, useViewer = F)"
  },
  {
    "objectID": "praat2emu.html#calculating-pitch-with-wrassp",
    "href": "praat2emu.html#calculating-pitch-with-wrassp",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.3 Calculating pitch with wrassp",
    "text": "4.3 Calculating pitch with wrassp\n\nThe summary() above showed that there are no SSFF tracks associated with the database. Recall from Chapter 2 that SSFF tracks store signal data that is timelocked to the audio signal. In this next step, we will generate SSFF tracks for praat_emuDB containing information about pitch.\nIn order to do this, we use the signal processing package wrassp. There are several different signal processing packages in R, and several of them have pitch tracking algorithms. We work with wrassp here because it was developed in conjunction with emuR.\nTo see the full range of signal processing routines available in wrassp, enter ?wrassp in the R console. There are two possible routines for calculating pitch: ksvF0 and mhsF0. For this demonstration, we will use mhsF0.\nThe function mhsF0 (and other signal processing functions in wrassp) operates directly on one or more .wav files, and creates corresponding new files containing pitch tracks. It does not care whether those .wav files are part of an Emu database, so to speak. In order to add SSFF tracks that are based on the signal processing capabilities in wrassp directly to an Emu database, we can use the handy emuR function add_ssffTrackDefinition() with the argument onTheFlyFunctionName. The onTheFlyFunctionName should be one of the signal processing routines in wrassp.\nHere’s how to use mhsF0 to generate pitch tracks with the default settings and add them to a Emu database:\n\nadd_ssffTrackDefinition(praat_DB, name='pitch', onTheFlyFunctionName='mhsF0',\n                        verbose=FALSE)\n\nIf we run list_ssffTrackDefinitions() we will now see that an SSFF track called pitch with the file extension .pit has been added:\n\nlist_ssffTrackDefinitions(praat_DB)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nChanging the pitch tracking parameters\n\n\n\nIt is often not a good idea to use the default parameters for pitch tracking. For example, mhsF0() and ksvF0() by default search for pitch in an extremely broad frequency range (between 50–600 Hz), and it can often be a good idea to narrow this down. We can see all possible mhsF0() parameters using the args() function, which returns all arguments of a given function:\n\nargs(mhsF0)\n\nfunction (listOfFiles = NULL, optLogFilePath = NULL, beginTime = 0, \n    centerTime = FALSE, endTime = 0, windowShift = 5, gender = \"u\", \n    maxF = 600, minF = 50, minAmp = 50, minAC1 = 0.25, minRMS = 18, \n    maxZCR = 3000, minProb = 0.52, plainSpectrum = FALSE, toFile = TRUE, \n    explicitExt = NULL, outputDirectory = NULL, forceToLog = useWrasspLogger, \n    verbose = TRUE) \nNULL\n\n\nAll of these arguments can be changed. If you want to change the settings when adding an SSFF track, you can pass a named list of parameters to the argument onTheFlyParams in the add_ssffTrackDefinition() call. A named list looks e.g. like this:\n\npitch_params &lt;- list(maxF=400, minF=75)\n\nThe function call to add pitch tracks with these parameters looks like this. Since we already have a track called pitch with the file extension pit, we call this one pitch2 and use the file extension pit2.\n\nadd_ssffTrackDefinition(praat_DB, name='pitch2', \n                        onTheFlyFunctionName='mhsF0',\n                        onTheFlyParams=pitch_params,\n                        fileExtension='pit2',\n                        verbose=FALSE)\n\n\n\n\nThe functions in wrassp are discussed in much more detail in Chapter 8."
  },
  {
    "objectID": "praat2emu.html#displaying-the-pitch-files-in-the-emu-webapp",
    "href": "praat2emu.html#displaying-the-pitch-files-in-the-emu-webapp",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.4 Displaying the pitch files in the Emu-webApp",
    "text": "4.4 Displaying the pitch files in the Emu-webApp\nAs seen in Chapter 2, we can use the function get_signalCanvasesOrder() to see which signals are currently displayed in the database:\n\nget_signalCanvasesOrder(praat_DB, perspectiveName = \"default\")\n\n[1] \"OSCI\" \"SPEC\"\n\n\nThis confirms that what is seen when viewing the database with the serve() function is the waveform (OSCI) and the spectrogram (SPEC). The pitch data created above now needs to be added using the function set_signalCanvasesOrder().\n\nset_signalCanvasesOrder(praat_DB, \n                        perspectiveName = \"default\",\n                        order = c(\"OSCI\", \"SPEC\", \"pitch\"))\n\n(Alternatively, if you have installed emuhelpeR, you can add it like so: praat_DB %&gt;% add_signal_canvas('pitch')).\nAnd the pitch track should now be visible when you serve():\n\nserve(praat_DB, useViewer = F)"
  },
  {
    "objectID": "praat2emu.html#adding-an-event-level",
    "href": "praat2emu.html#adding-an-event-level",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.5 Adding an event level",
    "text": "4.5 Adding an event level\nThe next task is to add an annotation level of type EVENT (similar to the point tiers in Praat) that can be used for labeling tones. We will call this level Tone. So far, the only existing time tier is ORT as confirmed with:\n\nlist_levelDefinitions(praat_DB)\n\n\n\n  \n\n\n\nIn order to add a new EVENT-type level called Tone, we use the function `add_levelDefinition like so:\n\nadd_levelDefinition(praat_DB, \n                    name=\"Tone\", \n                    type=\"EVENT\",\n                    verbose=FALSE)\n\nset_levelCanvasesOrder() can be used to ensure that Tone is shown when directly underneath the signals when serve()ing the database:\n\nset_levelCanvasesOrder(praat_DB, \n                       perspectiveName = \"default\", \n                       order = c(\"Tone\", \"ORT\"))"
  },
  {
    "objectID": "praat2emu.html#labelling-some-tones",
    "href": "praat2emu.html#labelling-some-tones",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.6 Labelling some tones",
    "text": "4.6 Labelling some tones\nTry and add two tone labels H* at the time of the pitch peaks of morgens and ruhig in the bundle wetter1 as in Figure 4.1 and save the result. The pointers for annotating given in Chapter 2 also apply here.\n\nserve(praat_DB, useViewer=F)\n\nAt present, there are no defined relationships between the different annotation levels, as confirmed with list_linkDefinitions():\n\nlist_linkDefinitions(praat_DB)\n\nNULL\n\n\nWe would like the tones we annotate to be linked to words within which they occur in time. To do this, we use add_linkDefinition() to define a hierarchical relationship such that ORT dominates Tone:\n\nadd_linkDefinition(praat_DB, \n                   type = \"ONE_TO_MANY\", \n                   superlevelName = \"ORT\", \n                   sublevelName = \"Tone\")\n\nlist_linkDefinitions(praat_DB)\n\n\n\n  \n\n\n\nTo see the results of this procedure, serve() the database again and switch to hierarchy view.\n\nserve(praat_DB, useViewer = F)"
  },
  {
    "objectID": "praat2emu.html#automatically-linking-event-and-segment-times",
    "href": "praat2emu.html#automatically-linking-event-and-segment-times",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.7 Automatically linking event and segment times",
    "text": "4.7 Automatically linking event and segment times\nOnce a hierarchical relationship has been established, between two levels, times can be linked using the autobuild_linkFromTimes() function, like so:\n\nautobuild_linkFromTimes(praat_DB,\n                        superlevelName = \"ORT\",\n                        sublevelName = \"Tone\",\n                        verbose=FALSE)\n\nTo see the results of this procedure, serve() the database again and switch to hierarchy view.\n\nserve(praat_DB, useViewer = F)\n\n\n\n\n\n\n\nWant to make plots in the style of Praat Picture?\n\n\n\nA very nice feature of Praat is that when you’ve found a nice portion of the signal, it’s fairly simple to make a plot that combines waveform, spectrogram, other derived tracks, and annotations. Say you’ve fallen in love with the portion in wetter3 in our praat_emuDB, and you want to plot the zu Samstag portion. Unfortunately, there are no functions internal to emuR that allows you to plot audio data along with annotations, but the package praatpicture allows this.\npraatpicture can be installed from GitHub like so, assuming that devtools is installed on your machine:\n\ndevtools::install_github('rpuggaardrode/praatpicture')\n\nThe function emupicture() can now be called to produce a Praat-style plot like so, setting the start and end time to plot our favorite part of the signal. The argument tg_tiers specifies which annotation levels (equivalent to TextGrid tiers should be plotted), and the wave_channels argument specifies which channels of the sound file should be plotted. (This is not visible in the Emu-webApp, but the sound files in praat_DB actually have two channels).\n\nlibrary(praatpicture)\nemupicture(praat_DB, bundle='wetter3', start=0.6, end=1.275, tg_tiers='ORT',\n           wave_channels=1)\n\n\n\n\nemupicture() is under development, but it is a quite flexible function with functionality that goes way beyond what was shown here. See the GitHub page for my information."
  },
  {
    "objectID": "praat2emu.html#functions-introduced-in-this-chapter",
    "href": "praat2emu.html#functions-introduced-in-this-chapter",
    "title": "4  Converting a Praat TextGrid collection",
    "section": "4.8 Functions introduced in this chapter",
    "text": "4.8 Functions introduced in this chapter\n\n\nconvert_TextGridCollection(): converts pairs of .wav and .TextGrid files to an Emu database.\nmhsF0() and ksvF0(): generates files with pitch track information from one or more sound files.\nadd_ssffTrackDefinition(): adds SSFF files with track information timelocked with the audio signal; often done on the fly with signal processing functions from wrassp\nadd_linkDefinition(): establishes a hierarchical relationship between two annotation levels\nautobuild_linkFromTimes(): uses time information to link annotation items on two tiers automatically\nemupicture(): makes plots combining signal and annotation in the style of Praat Picture (requires praatpicture to be installed)"
  },
  {
    "objectID": "forced_alignment.html#objective-and-preliminaries",
    "href": "forced_alignment.html#objective-and-preliminaries",
    "title": "5  Forced alignment in emuR",
    "section": "5.1 Objective and preliminaries",
    "text": "5.1 Objective and preliminaries\nThe objective of this chapter is to show how to go from a directory with .wav files and simple .txt files containing orthographic transcriptions to a phonetically annotated and force-aligned Emu database.\nThe assumption is that you already have an R project called ipsR and that it contains the directories emu_databases and testsample. If this is not the case, please go back and follow the preliminaries chapter.\n\nStart up R in the project you are using for this course and load the following packages:\n\nlibrary(tidyverse)\nlibrary(emuR)\nlibrary(wrassp)\n\nIn R, store the path to the directory testsample as sourceDir in the following way:\n\nsourceDir &lt;- \"./testsample\"\n\nAnd also store in R the path to emu_databases as targetDir:\n\ntargetDir &lt;- \"./emu_databases\""
  },
  {
    "objectID": "forced_alignment.html#converting-a-text-collection-into-an-emu-database",
    "href": "forced_alignment.html#converting-a-text-collection-into-an-emu-database",
    "title": "5  Forced alignment in emuR",
    "section": "5.2 Converting a text collection into an Emu database",
    "text": "5.2 Converting a text collection into an Emu database\nThe directory ./testsample/german on your computer contains .wav files and .txt files. Define the path to this directory in R and check that you can see these files with the list.files() function:\n\npath.german &lt;- file.path(sourceDir, \"german\")\nlist.files(path.german)\n\n[1] \"K01BE001.txt\" \"K01BE001.wav\" \"K01BE002.txt\" \"K01BE002.wav\"\n\n\nThe above is an example of a text collection because it contains matching .wav and .txt files in the same directory such that, for each .wav file, the .txt file contains the corresponding orthography. We can see that this is true by using the function read_file() to read the context of these .txt files:\n\nread_file(file.path(path.german, 'K01BE001.txt'))\n\n[1] \"heute ist schönes Frühlingswetter\"\n\nread_file(file.path(path.german, 'K01BE002.txt'))\n\n[1] \"die Sonne lacht\"\n\n\nThe command convert_txtCollection() is used to convert a text collection into an Emu database. Below we make an Emu database called ger2, which we’ll store in targetDir:\n\nconvert_txtCollection(dbName = \"ger2\",\n                      sourceDir = path.german,\n                      targetDir = targetDir,\n                      verbose=FALSE)\n\nLoad the database into R with load_emuDB():\n\nger2_DB &lt;- load_emuDB(file.path(targetDir, \"ger2_emuDB\"), verbose=FALSE)\nsummary(ger2_DB)\n\n\n\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ger2 \nUUID:    bcf2e700-ff87-48f9-9415-84203a11a290 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\ger2_emuDB \nSession count: 1 \nBundle count: 2 \nAnnotation item count:  2 \nLabel count:  4 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type nrOfAttrDefs attrDefNames          \n bundle ITEM 2            bundle; transcription;\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nserve() the database and have a look at it:\n\nserve(ger2_DB, useViewer = F)\n\nIf you switch to hierarchy view, you should see that the words in the .txt files are a single item in the attribute tier of bundle with the name transcription, as shown in the figure below:\n\nIt is evident when query()ing the database that the words are stored in this way, as shown below. Note that we need to include the argument calcTimes=FALSE here, because the annotation level transcription is of type ITEM and is not linked to a time-based level (i.e. a SEGMENT or an EVENT level).\n\nquery(ger2_DB, \"transcription =~ .*\", calcTimes=FALSE)"
  },
  {
    "objectID": "forced_alignment.html#forced-alignment",
    "href": "forced_alignment.html#forced-alignment",
    "title": "5  Forced alignment in emuR",
    "section": "5.3 Forced alignment",
    "text": "5.3 Forced alignment\nWe are now going to run the Munich Automatic Segmentation (MAUS) pipeline over the database. We do this with the function runBASwebservice_all(), which combines a number of online processing tools. Obligatory arguments are transcriptionAttributeDefinitionName() which will be the name of the newly created annotation level, and language, which in this case we set to deu-DE. We also set the argument runMINNI to FALSE; this is potentially used to forced-align data which has no annotations at all. Note that runBASwebservice_all() can only be used if you have an active internet connection.\n\nrunBASwebservice_all(ger2_DB,\n  transcriptionAttributeDefinitionName = \"transcription\",\n  language = \"deu-DE\", \n  runMINNI = FALSE,\n  verbose = FALSE)\n\n\n\n\n\n\n\nThe language setting in MAUS\n\n\n\nThere are quite a lot of languages available in MAUS. As of this writing, you can force-align Afrikaans, Albanian, Arabic, Basque, Catalan, Dutch, English, Estonian, Finnish, French, Georgian, German, Hungarian, Icelandic, Italian, Japanese, Luxembourgish, Maltese, Min Nan, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Swedish, and Thai. There’s also a language independent mode which expects files to be phonetically transcribed in X-SAMPA, and a special mode for Australian aboriginal languages. Additionally, many of these languages have modes for multiple different dialects. More information can be found in the MAUS help files and in the MAUS web interface.\n\n\nLet’s have a look at the database summary again:\n\nsummary(ger2_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    ger2 \nUUID:    bcf2e700-ff87-48f9-9415-84203a11a290 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\ger2_emuDB \nSession count: 1 \nBundle count: 2 \nAnnotation item count:  58 \nLabel count:  74 \nLink count:  52 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type    nrOfAttrDefs attrDefNames          \n bundle ITEM    2            bundle; transcription;\n ORT    ITEM    3            ORT; KAN; KAS;        \n MAU    SEGMENT 1            MAU;                  \n MAS    ITEM    1            MAS;                  \n\n\n── Link definitions ──\n\n\n type        superlevelName sublevelName\n ONE_TO_MANY bundle         ORT         \n ONE_TO_MANY ORT            MAS         \n ONE_TO_MANY MAS            MAU         \n\n\nNote that multiple extra levels and attributes have been created (ORT, KAN, KAS, MAU, and MAS), as well as links between them.\nLet’s serve() the database.\n\nserve(ger2_DB, useViewer=FALSE)\n\nWe immediately see that phone-level annotations have been added in the MAU level. Have a look at the hierarchy view and try to identify the levels, links, and attributes.\n\nThis shows that the phone-level annotations are linked to syllable-level annotations in the MAS level and word-level orthographic annotations in the ORT level. The ORT level further has the attributes KAN and KAS. These contain canonical representations of the word, i.e. phonetic annotations corresponding to the canonical pronunciations of these words. The MAS level is chunked into syllables.\nGiven this complex information, more complex queries are now also possible. Let’s say we want to find the word-initial MAU segments of all polysyllabic words:\n\nmau.s &lt;- query(ger2_DB,\n               \"[[MAU =~.* & Start(ORT, MAU)=1] ^ Num(ORT, MAS) &gt; 1]\")\nmau.s\n\n\n\n  \n\n\n\nThis data frame can then be passed to requery_hier() so we can see the labels in the ORT level associated with these words, like so:\n\nrequery_hier(ger2_DB, mau.s, \"ORT\")\n\n\n\n  \n\n\n\nThis may all seem rather opaque, but we’ll go into much more detail with how the querying language works in Chapter 7."
  },
  {
    "objectID": "forced_alignment.html#forced-alignment-albanian",
    "href": "forced_alignment.html#forced-alignment-albanian",
    "title": "5  Forced alignment in emuR",
    "section": "5.4 Forced alignment: Albanian",
    "text": "5.4 Forced alignment: Albanian\nNext we’ll try out forced alignment for a different language (Albanian) and we will show how forced alignment can be done from a canonical phonemic transcription instead of from text.\n\n5.4.1 From a text collection\nFirst we’ll use a text collection like we saw for German previously. This text collection is in our sourceDir in a folder called albanian:\n\npath.albanian &lt;- file.path(sourceDir, \"albanian\")\n\nFirst we’ll convert the text collection into an Emu database using convert_txtCollection() as above.\n\nconvert_txtCollection(dbName = \"alb\",\n                      sourceDir = path.albanian,\n                      targetDir = targetDir,\n                      verbose=FALSE)\n\nalb_DB &lt;- load_emuDB(file.path(targetDir, \"alb_emuDB\"), verbose=FALSE)\nsummary(alb_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    alb \nUUID:    dd0db274-68af-44fa-adf4-fe430f5af380 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\alb_emuDB \nSession count: 1 \nBundle count: 4 \nAnnotation item count:  4 \nLabel count:  8 \nLink count:  0 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type nrOfAttrDefs attrDefNames          \n bundle ITEM 2            bundle; transcription;\n\n\n── Link definitions ──\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\nHave a look at the database, switch to hierarchy view, and verify that the words have been located at bundle -&gt; transcription as for the German database above.\n\nserve(alb_DB, useViewer = F)\n\nNow run MAUS, just as before. The language code for Albanian is sqi-AL. Note that this will take longer than for German, possibly a couple of minutes.\n\nrunBASwebservice_all(alb_DB, \n                     transcriptionAttributeDefinitionName = \"transcription\",\n                     language = \"sqi-AL\", \n                     runMINNI = F,\n                     verbose=FALSE)\n\nsummary(alb_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    alb \nUUID:    dd0db274-68af-44fa-adf4-fe430f5af380 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\alb_emuDB \nSession count: 1 \nBundle count: 4 \nAnnotation item count:  138 \nLabel count:  176 \nLink count:  125 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type    nrOfAttrDefs attrDefNames          \n bundle ITEM    2            bundle; transcription;\n ORT    ITEM    3            ORT; KAN; KAS;        \n MAU    SEGMENT 1            MAU;                  \n MAS    ITEM    1            MAS;                  \n\n\n── Link definitions ──\n\n\n type        superlevelName sublevelName\n ONE_TO_MANY bundle         ORT         \n ONE_TO_MANY ORT            MAS         \n ONE_TO_MANY MAS            MAU         \n\n\nLook at the database and verify that the same kind of information has been automatically derived, as for the German database earlier.\n\nserve(alb_DB, useViewer = F)\n\n\n\n5.4.2 From a canonical representation\nMAUS also allows an automatic segmentation to be derived directly from the canonical level that we saw in the KAN attribute above. This can be useful when the canonical representation provided by MAUS deviates considerably from what was actually said. For one of the words in 0001BF_1syll_1, the canonical representation has J E when what was actually said was closer to n J E.\nFirst switch in hierarchy view from ORT → KAN and then change the node J E of the ORT:KAN level to n J E for file 0001BF_1syll_1 in the manner of Figure 5.1, as we also saw in Chapter 3.\n\n\n\nFigure 5.1: A fragment of a hierarchy view.\n\n\nIn order to run MAUS on this more appropriate pronunciation, first change it as in Figure 5.1 above, and don’t forget to save the annotation after editing. Now MAUS can be run directly on this canonical level using the runBASwebservice_maus() function. Here we again pass the language, and we pass the name of the existing annotation level with a canonical representation KAN to the argument canoAttributeDefinitionName and the name of the newly created force-aligned level mausAttributeDefinitionName, which we call MAU2 to differentiate it from the already created MAU tier.\n\nrunBASwebservice_maus(alb_DB,\n                      canoAttributeDefinitionName = \"KAN\",\n                      mausAttributeDefinitionName = \"MAU2\",\n                      language = \"sqi-AL\",\n                      verbose=FALSE)\n\nInspect the database again. There should now be a new tier MAU2.\n\nsummary(alb_DB)\n\n── Summary of emuDB ────────────────────────────────────────────────────────────\n\n\nName:    alb \nUUID:    dd0db274-68af-44fa-adf4-fe430f5af380 \nDirectory:   C:\\Users\\rasmu\\surfdrive\\emuintro\\emuintro\\emu_databases\\alb_emuDB \nSession count: 1 \nBundle count: 4 \nAnnotation item count:  219 \nLabel count:  257 \nLink count:  197 \n\n\n\n\n\n── Database configuration ──────────────────────────────────────────────────────\n\n\n\n\n\n── SSFF track definitions ──\n\n\n\n\n\ndataramme med 0 kolonner og 0 rækker\n\n\n── Level definitions ──\n\n\n name   type    nrOfAttrDefs attrDefNames          \n bundle ITEM    2            bundle; transcription;\n ORT    ITEM    3            ORT; KAN; KAS;        \n MAU    SEGMENT 1            MAU;                  \n MAS    ITEM    1            MAS;                  \n MAU2   SEGMENT 1            MAU2;                 \n\n\n── Link definitions ──\n\n\n type        superlevelName sublevelName\n ONE_TO_MANY bundle         ORT         \n ONE_TO_MANY ORT            MAS         \n ONE_TO_MANY MAS            MAU         \n ONE_TO_MANY ORT            MAU2        \n\n\nIf you have reason to suspect that the canonical representation will differ from what is actually said in the recording, you can use the function runBASwebserivce_g2pForPronunciation() as the first step. This way, you will not have to run MAUS twice, as runBASwebserivce_g2pForPronunciation() only generates the canonical representations without doing forced alignment. If you have much more data, this could possibly speed up your process."
  },
  {
    "objectID": "forced_alignment.html#functions-introduced-in-this-chapter",
    "href": "forced_alignment.html#functions-introduced-in-this-chapter",
    "title": "5  Forced alignment in emuR",
    "section": "5.5 Functions introduced in this chapter",
    "text": "5.5 Functions introduced in this chapter\n\nread_file(): reads the content of a .txt file into R\nconvert_txtCollection(): converts pairs of .wav and .txt files into an Emu database.\nrun_BASwebservice_all(): performs all the steps needed to get from an Emu database with orthographical transcription that aren’t time-aligned to a multi-level force-aligned phonetic annotation\nrun_BASwebservice_maus(): performs forced alignment on an Emu database which already has (non-time-aligned) canonical phonetic annotations\nrun_BASwebservice_g2pForPronunciation(): performs grapheme-to-phoneme (G2P) conversion of an orthographical transcription to a canonical phonetic transcription on an Emu database"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baayen, R. Harald. 2008. Analyzing Linguistic Data. A Practical\nIntroduction to Statistics Using r. Cambridge: Cambridge University\nPress. https://doi.org/10.1017/CBO9780511801686.\n\n\nBombien, Lasse, Steve Cassidy, Jonathan Harrington, Tina John, and\nSallyanne Palethorpe. 2006. “Recent Developments in the Emu Speech\nDatabase System.” Proceedings of the Australian Speech\nScience and Technology Conference. https://www.phonetik.uni-muenchen.de/~jmh/papers/emusst06.pdf.\n\n\nCassidy, Steve, and Jonathan Harrington. 2001. “Multi-Level\nAnnotation in the Emu Speech Database Management System.”\nSpeech Communication 33 (1–2): 61–77. https://doi.org/10.1016/S0167-6393(00)00069-8.\n\n\nGries, Stefan Th. 2021. Statistics for Linguistics with r. A\nPractical Introduction. 3rd ed. Berlin & Boston: De Gruyter\nMouton. https://doi.org/10.1515/9783110718256.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic\nData. Cambridge, MA: MIT Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed. Sebastopol, CA: O’Reilly Media.\n\n\nWinkelmann, Raphael, Jonathan Harrington, and Klaus Jänsch. 2017.\n“EMU-SDMS. Advanced Speech Database\nManagement and Analysis in r.” Computer Speech &\nLanguage 45: 392–410. https://doi.org/10.1016/j.csl.2017.01.002.\n\n\nWinter, Bodo. 2019. Statistics for Linguists. An Introduction Using\nr. New York & London: Routledge. https://doi.org/10.4324/9781315165547."
  }
]